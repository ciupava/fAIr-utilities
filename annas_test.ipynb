{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests on YoloV8 for fAIr\n",
    "Using model V2 from Omdena results.\n",
    "Mocking `test_yolo_v2.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import ultralytics\n",
    "import yaml\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "os.environ.update(os.environ)\n",
    "os.environ[\"RAMP_HOME\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader imports\n",
    "from hot_fair_utilities import polygonize, predict, preprocess\n",
    "from hot_fair_utilities.preprocessing.yolo_v8_v2.yolo_format_anna import yolo_format\n",
    "from hot_fair_utilities.training.yolo_v8_v2 import train as train_yolo\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic variables\n",
    "# base_path = f\"{os.getcwd()}/ramp-data/sample_2\"\n",
    "base_path = '/Users/azanchetta/fAIr_metric'\n",
    "data_path = f'{base_path}/training_results'\n",
    "preprocessed_ramp_data_path=f'{base_path}/metric_data'\n",
    "k_data_path = f'{base_path}/anna-dataset' # this has been added for dealing with Kshiitj's data\n",
    "#  Obtain cities list folders name in data folder\n",
    "# cities_list = ['modelfake', 'model149_td489'] # sample of names, for tests\n",
    "cities_list= [ item for item in os.listdir(preprocessed_ramp_data_path) if os.path.isdir(os.path.join(preprocessed_ramp_data_path, item)) ]\n",
    "datasets_list = cities_list= [ item for item in os.listdir(k_data_path) if os.path.isdir(os.path.join(k_data_path, item)) ]  # this has been added for dealing with Kshiitj's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cities_list))\n",
    "for i in cities_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class print_time:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        print(f\"{self.name} took {round(time.perf_counter() - self.start, 2)} seconds\")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ONLY RUN THE CELL BELOW ONCE\n",
    "\n",
    "## Generate Yolo format input files\n",
    "Note: need to re-run the preprocessing, can't use the preprocessed Ramp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem with the data, as in the only data I have is the ramp preprocessed one. I can't run the yolo pre-processing on my own, so Kshiitj sent preprocessed data (he done that in the backend).\n",
    "\n",
    "`anna-dataset` has this structure:\n",
    "```model name\n",
    "    |\n",
    "     - preprocessed\n",
    "        |\n",
    "         - binarymasks/\n",
    "         - chips/\n",
    "         - inputs/\n",
    "         - labels/\n",
    "     - yolo_v1 \n",
    "        |\n",
    "         - images/\n",
    "         - labels/\n",
    "         - yolo_dataset.yaml\n",
    "```\n",
    "\n",
    "I need to restructure this to be consistent with my folder structure... or could just decide to use it like it is :P\n",
    "\n",
    "**IMPORTANT** I need to rerun the preprocessing anyways, because the division train/val/test must be the same as ramp\n",
    "\n",
    "--- renaming the folder `yolo_v2_dataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  importing csv file, for LUT with models / training dataset / dataset codes\n",
    "lut_csvfile = f'{base_path}/cities_lut.csv'\n",
    "# with open(lut_csvfile) as csv_file:\n",
    "#     lut = csv.reader(csv_file)\n",
    "\n",
    "lut = pd.read_csv(lut_csvfile,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_model</th>\n",
       "      <th>id_train</th>\n",
       "      <th>ds_size</th>\n",
       "      <th>urban_region</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>id_dataset</th>\n",
       "      <th>urban_type</th>\n",
       "      <th>density</th>\n",
       "      <th>roof_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>364</td>\n",
       "      <td>399</td>\n",
       "      <td>Kakuma</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>Africa</td>\n",
       "      <td>58</td>\n",
       "      <td>refugee camp</td>\n",
       "      <td>sparse</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>95</td>\n",
       "      <td>370</td>\n",
       "      <td>168</td>\n",
       "      <td>Denver</td>\n",
       "      <td>USA</td>\n",
       "      <td>America North</td>\n",
       "      <td>135</td>\n",
       "      <td>peri-urban</td>\n",
       "      <td>grid</td>\n",
       "      <td>shingles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>372</td>\n",
       "      <td>420</td>\n",
       "      <td>Montevideo</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>America South</td>\n",
       "      <td>137</td>\n",
       "      <td>urban</td>\n",
       "      <td>grid</td>\n",
       "      <td>cement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>98</td>\n",
       "      <td>373</td>\n",
       "      <td>399</td>\n",
       "      <td>Montevideo dense</td>\n",
       "      <td>Uruguay</td>\n",
       "      <td>America South</td>\n",
       "      <td>138</td>\n",
       "      <td>urban</td>\n",
       "      <td>dense</td>\n",
       "      <td>cement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>102</td>\n",
       "      <td>391</td>\n",
       "      <td>231</td>\n",
       "      <td>Kutupalong</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>Asia</td>\n",
       "      <td>144</td>\n",
       "      <td>refugee camp</td>\n",
       "      <td>dense</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  id_model  id_train  ds_size      urban_region     country  \\\n",
       "0   1        51       364      399            Kakuma       Kenya   \n",
       "1   2        95       370      168            Denver         USA   \n",
       "2   3        97       372      420        Montevideo     Uruguay   \n",
       "3   4        98       373      399  Montevideo dense     Uruguay   \n",
       "4   5       102       391      231        Kutupalong  Bangladesh   \n",
       "\n",
       "       continent  id_dataset    urban_type density roof_type  \n",
       "0         Africa          58  refugee camp  sparse     metal  \n",
       "1  America North         135    peri-urban    grid  shingles  \n",
       "2  America South         137         urban    grid    cement  \n",
       "3  America South         138         urban   dense    cement  \n",
       "4           Asia         144  refugee camp   dense     mixed  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lut.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = lut.astype(str) # convert them all to string, for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  '/Users/azanchetta/fAIr_metric/training_results/model108_td385/train/fair_split_train.csv'\"\n",
    "#  /Users/azanchetta/fAIr_metric/training_results/model162_td519/train/fair_split_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dataset in enumerate(datasets_list):\n",
    "    print(f'original name to be split {dataset}')\n",
    "    dataset_name = dataset.split(\"_\")[1]\n",
    "    print(f'{idx}) dataset {dataset} and dataset_name {dataset_name}')\n",
    "    model_name = lut.loc[lut['id_dataset'] == dataset_name, 'id_model'].values[0] # without the values bit, you get a whole piece of dataframe\n",
    "    td_name = lut.loc[lut['id_dataset'] == dataset_name, 'id_train'].values[0]\n",
    "    print(f'model {model_name}, td {td_name}, dataset {dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities_list = ['modelfake', 'model149_td489'] # sample of names, for tests\n",
    "# datasets_list = ['dataset_205']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the cities list, and check per each folder that tiles number is consistent, and the shapes too\n",
    "# for city in cities_list:\n",
    "for dataset in datasets_list:\n",
    "    dataset_name = dataset.split(\"_\")[1]\n",
    "\n",
    "    model_name = lut.loc[lut['id_dataset'] == dataset_name, 'id_model'].values[0] # without the values bit, you get a whole piece of dataframe\n",
    "    td_name = lut.loc[lut['id_dataset'] == dataset_name, 'id_train'].values[0]\n",
    "    print(f'_________\\nDataset {dataset}, model {model_name}, training dataset {td_name}\\n')\n",
    "    \n",
    "    city = f'model{model_name}_td{td_name}'\n",
    "    # city_folder_name=f'{base_path}/metric_data/{city}'\n",
    "    \n",
    "    csv_file_basepath = f'{data_path}/{city}/train'\n",
    "    print(f'\\n---\\nNow working on {city}\\n---')\n",
    "    # print(f'city folder name is {city_folder_name}\\n\\n---\\n')\n",
    "\n",
    "    # model_input_image_path = f\"{base_path}/input\"\n",
    "    # preprocess_output = f\"{base_path}/preprocessed\"\n",
    "    # with print_time(\"preprocessing\"):\n",
    "    #     preprocess(\n",
    "    #         input_path=model_input_image_path,\n",
    "    #         output_path=preprocess_output,\n",
    "    #         rasterize=True,\n",
    "    #         rasterize_options=[\"binary\"],\n",
    "    #         georeference_images=True,\n",
    "    #         multimasks=False,\n",
    "    #         epsg=4326\n",
    "    #     )\n",
    "\n",
    "    # city_data_dir = f'{base_path}/training_results/{city}/train'\n",
    "    # city_data_dir = f'{city_folder_name}'\n",
    "    dataset_foldername = f'dataset_{dataset_name}'\n",
    "    city_data_dir = f'{k_data_path}/{dataset_foldername}/preprocessed' # name as it appears in kshitij's folder\n",
    "    \n",
    "    yolo_data_dir = f'{base_path}/yolo_v2_preprocessed' # name for the output\n",
    "    print(f'city is {city}')\n",
    "    with print_time(\"yolo conversion\"):\n",
    "        print(f'\\n___ Starting yolo files conversion\\n')\n",
    "        yolo_format(\n",
    "            input_path=city_data_dir,\n",
    "            csv_path=csv_file_basepath,\n",
    "            output_path=yolo_data_dir,\n",
    "            city_name=city\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities_list = ['modelfake', 'model149_td489'] # sample of names, for tests\n",
    "cities_list = ['model149_td489'] #['modelfake'] # sample of names, for tests\n",
    "# cities_list= [ item for item in os.listdir(preprocessed_data_path) if os.path.isdir(os.path.join(preprocessed_data_path, item)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone: s, Dataset: yolo_v2_preprocessed, Epochs: 20\n",
      "New https://pypi.org/project/ultralytics/8.3.48 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.26 ðŸš€ Python-3.12.4 torch-2.2.2 CPU (Intel Core(TM) i9-9980HK 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=/Users/azanchetta/fAIr-utilities/yolov8s_v2-seg.pt, data=/Users/azanchetta/fAIr_metric/yolo_v2_preprocessed/model149_td489/yolo_dataset.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=256, save=True, save_period=-1, cache=True, device=cpu, workers=8, project=/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints, name=yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=False, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=False, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.00854, lrf=0.01232, momentum=0.95275, weight_decay=0.00058, warmup_epochs=3.82177, warmup_momentum=0.81423, warmup_bias_lr=0.1, box=7.48109, cls=0.775, dfl=1.5, pose=2.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.01269, hsv_s=0.68143, hsv_v=0.27, degrees=15.75, translate=0, scale=0, shear=0, perspective=0.0, flipud=0.5, fliplr=0.255, bgr=0.0, mosaic=0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2770931  ultralytics.nn.modules.head.Segment          [1, 32, 128, [128, 256, 512]] \n",
      "YOLOv8s-seg summary: 261 layers, 11,790,483 parameters, 11,790,467 gradients, 42.7 GFLOPs\n",
      "\n",
      "Transferred 417/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/azanchetta/fAIr_metric/yolo_v2_preprocessed/model149_td489/labels/train.cache... 103 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 1401.19it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/azanchetta/fAIr_metric/yolo_v2_preprocessed/model149_td489/labels/val.cache... 22 images, 5 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 1611.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.00854' and 'momentum=0.95275' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.00058), 76 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 256 train, 256 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      1.079      2.027      3.311      1.141         31        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.745      0.718      0.773      0.634      0.722      0.745       0.78      0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G     0.9099      1.603      2.364      1.042         17        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.763      0.587       0.72       0.59      0.648      0.709      0.732      0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20         0G     0.8467      1.418        1.9       1.02         74        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.782      0.536      0.694      0.551      0.673      0.609      0.706      0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20         0G     0.7982      1.292      1.719     0.9857         50        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.769      0.591      0.737      0.579      0.789      0.578      0.741      0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20         0G     0.7896      1.285      1.736     0.9957         21        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.721      0.755      0.796      0.638      0.721      0.755      0.807      0.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20         0G     0.7635      1.236      1.461     0.9566         52        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.739      0.782        0.8      0.653      0.739      0.782      0.804       0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20         0G     0.7508      1.136       1.25     0.9462         40        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:16<00:00,  2.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.755      0.814      0.823      0.656      0.755      0.814      0.823      0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20         0G     0.7524      1.211      1.222     0.9531         89        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.784       0.76      0.835       0.66      0.753      0.803      0.836      0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20         0G     0.7141      1.106      1.154     0.9442         78        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.779      0.773      0.837      0.671      0.775      0.785      0.849      0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20         0G     0.7243      1.103      1.108     0.9359         47        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:16<00:00,  2.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110        0.8      0.773      0.829      0.669        0.8      0.773      0.841      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20         0G     0.6992       1.03      1.081     0.9394         33        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.853      0.687      0.834      0.678      0.853      0.687      0.839      0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20         0G     0.6775      1.034      1.047     0.9243         55        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.832      0.721      0.841      0.686      0.832      0.721      0.845      0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20         0G     0.6818      1.021      1.011     0.9308         51        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.869      0.723      0.847      0.684      0.869      0.723      0.851      0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20         0G      0.683      1.063      1.033     0.9506         31        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.872      0.745      0.849      0.692      0.872      0.745      0.855      0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20         0G     0.6588      1.023      1.007     0.9158         95        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.791      0.809      0.852      0.684      0.791      0.809      0.858      0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20         0G     0.6525      1.001      1.026     0.9398         11        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:16<00:00,  2.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.815      0.773      0.843      0.675      0.824      0.782      0.856      0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20         0G     0.6631     0.9446      1.077     0.9093         23        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.791      0.791      0.845      0.674        0.8        0.8      0.858      0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20         0G     0.6487     0.9699     0.9644     0.9162         23        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.823      0.762      0.846      0.677      0.833      0.771       0.86      0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20         0G     0.6554     0.9879     0.9563     0.9198         27        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110       0.81      0.782      0.848      0.689       0.82      0.791      0.862      0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20         0G     0.6411     0.9411     0.9172     0.9245         35        256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  2.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.826      0.777      0.848      0.692      0.836      0.785      0.862      0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.101 hours.\n",
      "Optimizer stripped from /Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights/last.pt, 23.8MB\n",
      "Optimizer stripped from /Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights/best.pt, 23.8MB\n",
      "\n",
      "Validating /Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights/best.pt...\n",
      "Ultralytics 8.3.26 ðŸš€ Python-3.12.4 torch-2.2.2 CPU (Intel Core(TM) i9-9980HK 2.40GHz)\n",
      "YOLOv8s-seg summary (fused): 195 layers, 11,779,987 parameters, 0 gradients, 42.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.826      0.776      0.848      0.692      0.836      0.786      0.862      0.653\n",
      "Speed: 0.4ms preprocess, 52.3ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0\u001b[0m\n",
      "Ultralytics 8.3.26 ðŸš€ Python-3.12.4 torch-2.2.2 CPU (Intel Core(TM) i9-9980HK 2.40GHz)\n",
      "YOLOv8s-seg summary (fused): 195 layers, 11,779,987 parameters, 0 gradients, 42.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/azanchetta/fAIr_metric/yolo_v2_preprocessed/model149_td489/labels/val.cache... 22 images, 5 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         22        110      0.826      0.776      0.848      0.692      0.836      0.786      0.862      0.648\n",
      "Speed: 0.8ms preprocess, 115.0ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/azanchetta/fAIr-utilities/runs/segment/val9\u001b[0m\n",
      "Ultralytics 8.3.26 ðŸš€ Python-3.12.4 torch-2.2.2 CPU (Intel Core(TM) i9-9980HK 2.40GHz)\n",
      "YOLOv8s-seg summary (fused): 195 layers, 11,779,987 parameters, 0 gradients, 42.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights/best.pt' with input shape (1, 3, 256, 256) BCHW and output shape(s) ((1, 37, 1344), (1, 32, 64, 64)) (22.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 5.7s, saved as '/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights/best.onnx' (45.0 MB)\n",
      "\n",
      "Export complete (6.3s)\n",
      "Results saved to \u001b[1m/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights\u001b[0m\n",
      "Predict:         yolo predict task=segment model=/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights/best.onnx imgsz=256  \n",
      "Validate:        yolo val task=segment model=/Users/azanchetta/fAIr_metric/yolo_v2_training/model149_td489/checkpoints/yolov8s-seg_yolo_v2_preprocessed_ep20_bs8_pc2.0/weights/best.onnx imgsz=256 data=/Users/azanchetta/fAIr_metric/yolo_v2_preprocessed/model149_td489/yolo_dataset.yaml  \n",
      "Visualize:       https://netron.app\n",
      "68.03989639043021\n",
      "yolo training took 391.42 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "yolo_output_path = f'{base_path}/yolo_v2_training'\n",
    "yolo_data_dir = f'{base_path}/yolo_v2_preprocessed'\n",
    "with print_time(\"yolo training\"):\n",
    "    for city in cities_list:\n",
    "        city_yolodata_path = f'{base_path}/yolo_v2_preprocessed/{city}'\n",
    "        city_output_path = f'{yolo_output_path}/{city}'\n",
    "        # deal with the yaml file:\n",
    "        basic_yaml_file_name_with_path = '/Users/azanchetta/fAIr-utilities/ramp-data/sample_2/yolo_v2/yolo_dataset.yaml'\n",
    "        yaml_file_path_for_city = f'{city_yolodata_path}/yolo_dataset.yaml'\n",
    "        # print(f'name that will used to create a new yaml file for the city: {yaml_file_path_for_city}')\n",
    "        info_to_write_in_yaml = city_yolodata_path\n",
    "\n",
    "        with open(basic_yaml_file_name_with_path, 'r') as file:\n",
    "            yamlfile = yaml.safe_load(file)\n",
    "        yamlfile['path'] = city_yolodata_path\n",
    "        with open(yaml_file_path_for_city, 'w') as file:  # Save the updated YAML file\n",
    "            yaml.dump(yamlfile, file)\n",
    "\n",
    "        # Run the training\n",
    "        output_model_path,output_model_iou_accuracy = train_yolo(\n",
    "            data=city_yolodata_path, #f\"{base_path}\",\n",
    "            weights=f\"{os.getcwd()}/yolov8s_v2-seg.pt\", \n",
    "            # gpu=\"cpu\",\n",
    "            epochs=20,\n",
    "            batch_size=8,\n",
    "            pc=2.0,\n",
    "            output_path=city_output_path,\n",
    "            dataset_yaml_path=yaml_file_path_for_city #'/Users/azanchetta/fAIr_metric/yolo_v2_data/model51_td364/yolo_dataset.yaml'\n",
    "            # dataset_yaml_path='/Users/azanchetta/fAIr-utilities/ramp-data/sample_2/yolo_v2/yolo_dataset.yaml' ## this name is just a placeholder, we overwrite the variables in the code\n",
    "        )\n",
    "        print(output_model_iou_accuracy)\n",
    "\n",
    "        # output_model_path,output_model_iou_accuracy = train_yolo(\n",
    "        #     data=f\"{base_path}\",\n",
    "        #     weights=f\"{os.getcwd()}/yolov8s_v2-seg.pt\", \n",
    "        #     # gpu=\"cpu\",\n",
    "        #     epochs=2,\n",
    "        #     batch_size=16,\n",
    "        #     pc=2.0,\n",
    "        #     output_path=yolo_data_dir,\n",
    "        #     dataset_yaml_path=os.path.join(yolo_data_dir,'yolo_dataset.yaml')\n",
    "        # )\n",
    "        # print(output_model_iou_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "yolo_output_path = f'{base_path}/yolo_v2_predictions'\n",
    "yolo_data_dir = f'{base_path}/yolo_v2'\n",
    "with print_time(\"yolo training\"):\n",
    "    for city in cities_list:\n",
    "        city_yolodata_path = f'{base_path}/yolo_v2_data/{city}'\n",
    "        print(f'city path: {city_yolodata_path}')\n",
    "        output_model_path,output_model_iou_accuracy = train_yolo(\n",
    "            data=city_yolodata_path, #f\"{base_path}\",\n",
    "            weights=f\"{os.getcwd()}/yolov8s_v2-seg.pt\", \n",
    "            # gpu=\"cpu\",\n",
    "            epochs=2,\n",
    "            batch_size=16,\n",
    "            pc=2.0,\n",
    "            output_path=yolo_output_path,\n",
    "            dataset_yaml_path='/Users/azanchetta/fAIr-utilities/ramp-data/sample_2/yolo_v2/yolo_dataset.yaml' ## this name is just a placeholder, we overwrite the variables in the code\n",
    "        )\n",
    "        print(output_model_iou_accuracy)\n",
    "\n",
    "        # output_model_path,output_model_iou_accuracy = train_yolo(\n",
    "        #     data=f\"{base_path}\",\n",
    "        #     weights=f\"{os.getcwd()}/yolov8s_v2-seg.pt\", \n",
    "        #     # gpu=\"cpu\",\n",
    "        #     epochs=2,\n",
    "        #     batch_size=16,\n",
    "        #     pc=2.0,\n",
    "        #     output_path=yolo_data_dir,\n",
    "        #     dataset_yaml_path=os.path.join(yolo_data_dir,'yolo_dataset.yaml')\n",
    "        # )\n",
    "        # print(output_model_iou_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "prediction_output = f\"{base_path}/prediction/output\"\n",
    "# model_path = f\"{output_path}/weights/best.pt\"\n",
    "with print_time(\"inference\"):\n",
    "    predict(\n",
    "        checkpoint_path=output_model_path,\n",
    "        input_path=f\"{base_path}/prediction/input\",\n",
    "        prediction_path=prediction_output,\n",
    "    )\n",
    "\n",
    "geojson_output = f\"{prediction_output}/prediction.geojson\"\n",
    "with print_time(\"polygonization\"):\n",
    "    polygonize(\n",
    "        input_path=prediction_output,\n",
    "        output_path=geojson_output,\n",
    "        remove_inputs=False,\n",
    "    )\n",
    "\n",
    "print(f\"\\n Total Process Completed in : {time.time()-start_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deal with the csv files with list of train/val/pred images used in RAMP\n",
    "# #  testing function ... this goes inside `fined_files` in yolo_format_anna.py\n",
    "# city=\"model51_td364\"\n",
    "# city_folder_name=f'{data_path}/{city}/train'\n",
    "# csv_file_name = f'fair_split_train.csv'\n",
    "# csv_file_path = f'{city_folder_name}/{csv_file_name}'\n",
    "# print(f'CSV file is {csv_file_name}')\n",
    "# print(f'CSV file is {csv_file_path}')\n",
    "# csv_raw_list = []\n",
    "\n",
    "# with open(csv_file_path, \"r\") as file_obj:\n",
    "#     heading = next(file_obj)\n",
    "#     reader_obj = csv.reader(file_obj, delimiter=\"\\t\")\n",
    "#     for row in reader_obj:\n",
    "#         csv_raw_list.append(row)\n",
    "# print(f'this is the list from the csv file:\\n{csv_raw_list}')\n",
    "# csv_nested_list = []\n",
    "# for ccc in csv_raw_list:\n",
    "#     # print(ccc)\n",
    "#     nested = ccc[0]\n",
    "#     # print(f'nested {nested}')\n",
    "#     name_csv = nested.split('/')[-1]\n",
    "#     csv_nested_list.append(name_csv)\n",
    "\n",
    "# # filenames_from_csv = [csvi.split(\"/\",1)[-1] for csvi in csv_nested_list] # this is to get the last element of the string (i.e. the file name)\n",
    "# # print(f'filenames hopefully {filenames_from_csv}')\n",
    "# print(f'is this the names? {csv_nested_list}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kshitij-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
