{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building from `sam_test.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':/home/annazan/miniconda3/envs/fair38/lib/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getenv(\"LD_LIBRARY_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ":/home/annazan/miniconda3/envs/fair38/lib/\n",
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ":/home/annazan/miniconda3/envs/fair38/lib/\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH\n",
    "!source ~/.bashrc\n",
    "!echo $LD_LIBRARY_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WI8nslrSgROI",
    "outputId": "2cbad97c-765f-4d2b-87df-773447500332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/annazan/fAIr-utilities\n",
      "/home/annazan/fAIr-utilities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "os.environ.update(os.environ)\n",
    "        # Add a new environment variable to the operating system\n",
    "os.environ[\"RAMP_HOME\"] = os.getcwd()\n",
    "# Print the environment variables to verify that the new variable was added\n",
    "print(os.environ[\"RAMP_HOME\"])\n",
    "sys.path.append('../')\n",
    "sys.path.append('../ramp-code/')\n",
    "sys.path.append('ramp-code')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSM_FRAMEWORK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mramp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhot_fair_utilities\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# base_path = f\"{os.getcwd()}/ramp-data/sample_2\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# base_path = \"/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# base_path = f\"{os.getcwd()}/ramp-data/test_data/1_Zanzibar\"\u001b[39;00m\n",
      "File \u001b[0;32m~/fAIr-utilities/ramp-code/ramp/utils/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label_utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log_fields\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lrfinder\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mask_to_vec_utils\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m misc_ramp_utils\n",
      "File \u001b[0;32m~/fAIr-utilities/ramp-code/ramp/utils/lrfinder.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LambdaCallback\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/pyplot.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m docstring\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase, MouseButton\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Figure, figaspect\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgridspec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rcParams, rcParamsDefault, get_backend, rcParamsOrig\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/figure.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _blocking_input, docstring, projections\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     Artist, allow_rasterization, _finalize_rasterization)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/projections/__init__.py:56\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mNon-separable transforms that map from data space to screen space.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m`matplotlib.projections.polar` may also be of interest.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m axes, docstring\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolarAxes\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/projections/geo.py:299\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_core_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, resolution):\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAitoffTransform(resolution)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHammerAxes\u001b[39;00m(GeoAxes):\n\u001b[1;32m    300\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhammer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHammerTransform\u001b[39;00m(_GeoTransform):\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/artist.py:119\u001b[0m, in \u001b[0;36mArtist.__init_subclass__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_set_signature_and_docstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/artist.py:147\u001b[0m, in \u001b[0;36mArtist._update_set_signature_and_docstring\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m Signature(\n\u001b[1;32m    138\u001b[0m     [Parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, Parameter\u001b[38;5;241m.\u001b[39mPOSITIONAL_OR_KEYWORD),\n\u001b[1;32m    139\u001b[0m      \u001b[38;5;241m*\u001b[39m[Parameter(prop, Parameter\u001b[38;5;241m.\u001b[39mKEYWORD_ONLY, default\u001b[38;5;241m=\u001b[39m_UNSET)\n\u001b[1;32m    140\u001b[0m        \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m ArtistInspector(\u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mget_setters()\n\u001b[1;32m    141\u001b[0m        \u001b[38;5;28;01mif\u001b[39;00m prop \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m Artist\u001b[38;5;241m.\u001b[39m_PROPERTIES_EXCLUDED_FROM_SET]])\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m_autogenerated_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSet multiple properties at once.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported properties are\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[43mkwdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/artist.py:1749\u001b[0m, in \u001b[0;36mkwdoc\u001b[0;34m(artist)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03mInspect an `~matplotlib.artist.Artist` class (using `.ArtistInspector`) and\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03mreturn information about its settable properties and their current values.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;124;03m    use in Sphinx) if it is True.\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m ai \u001b[38;5;241m=\u001b[39m ArtistInspector(artist)\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ai\u001b[38;5;241m.\u001b[39mpprint_setters_rest(leadingspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocstring.hardcopy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m-> 1749\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProperties:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpprint_setters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleadingspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/artist.py:1509\u001b[0m, in \u001b[0;36mArtistInspector.pprint_setters\u001b[0;34m(self, prop, leadingspace)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (pad, prop, accepts)\n\u001b[1;32m   1508\u001b[0m lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_setters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m   1510\u001b[0m     accepts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_valid_values(prop)\n\u001b[1;32m   1511\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maliased_name(prop)\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/artist.py:1436\u001b[0m, in \u001b[0;36mArtistInspector.get_setters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo, name)\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func)\n\u001b[1;32m   1435\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(func)\u001b[38;5;241m.\u001b[39mparameters) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m-> 1436\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_alias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m setters\u001b[38;5;241m.\u001b[39mappend(name[\u001b[38;5;241m4\u001b[39m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/matplotlib/artist.py:1443\u001b[0m, in \u001b[0;36mArtistInspector.is_alias\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_alias\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return whether method object *o* is an alias for another method.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/inspect.py:623\u001b[0m, in \u001b[0;36mgetdoc\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcleandoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/inspect.py:646\u001b[0m, in \u001b[0;36mcleandoc\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    644\u001b[0m     lines[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m lines[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlstrip()\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m margin \u001b[38;5;241m<\u001b[39m sys\u001b[38;5;241m.\u001b[39mmaxsize:\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(lines)): lines[i] \u001b[38;5;241m=\u001b[39m \u001b[43mlines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[margin:]\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Remove any trailing or leading blank lines.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m lines \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import cv2\n",
    "import ramp.utils\n",
    "import hot_fair_utilities\n",
    "\n",
    "# base_path = f\"{os.getcwd()}/ramp-data/sample_2\"\n",
    "# base_path = \"/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\"\n",
    "# base_path = f\"{os.getcwd()}/ramp-data/test_data/1_Zanzibar\"\n",
    "base_path = f'{os.getcwd()}/ramp-data/test_data/model95_td370'\n",
    "model_input_image_path = f\"{base_path}/input\"\n",
    "# preprocess_output=f\"{base_path}/preprocessed\"\n",
    "preprocess_output=f\"{base_path}\"\n",
    "train_output = f\"{base_path}/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output\n",
    "# preprocess_output\n",
    "# model_input_image_path\n",
    "# base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hot_fair_utilities import preprocess\n",
    "\n",
    "# preprocess(\n",
    "#     input_path = model_input_image_path,\n",
    "#     output_path = preprocess_output,\n",
    "#     rasterize=True,\n",
    "#     # rasterize_options=[\"binary\"],\n",
    "#     rasterize_options=[\"binary\"],\n",
    "#     georeference_images=True,\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make multimask path\n",
    "# !mkdir ramp-data/test_data/1_Zanzibar/preprocessed/multimasks/\n",
    "# # Run script for multi-mask: https://github.com/kshitijrajsharma/ramp-code-fAIr/blob/ae33b11364f0a61f278ce9ff93446586704ea275/scripts/multi_masks_from_polygons.py\n",
    "# !python ramp-code/scripts/multi_masks_from_polygons.py -in_vecs ramp-data/test_data/1_Zanzibar/preprocessed/labels/ -in_chips ramp-data/test_data/1_Zanzibar/preprocessed/chips/ -out ramp-data/test_data/1_Zanzibar/preprocessed/multimasks/ -bwidth 2 -csp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_output=f\"{base_path}\"\n",
    "\n",
    "# from hot_fair_utilities.training.prepare_data import split_training_2_validation\n",
    "# x = split_training_2_validation(preprocess_output, train_output)\n",
    "\n",
    "# --- TO DO edits for split training to prediction data too\n",
    "from hot_fair_utilities.training.prepare_data import split_training_2_validation_2_prediction\n",
    "x = split_training_2_validation_2_prediction(preprocess_output, train_output)\n",
    "#  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from ramp.training import (\n",
    "    callback_constructors,\n",
    "    loss_constructors,\n",
    "    metric_constructors,\n",
    "    model_constructors,\n",
    "    optimizer_constructors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hot_fair_utilities.training.run_training import manage_fine_tuning_config\n",
    "\n",
    "output_path=train_output\n",
    "epoch_size=2\n",
    "batch_size=2\n",
    "freeze_layers=False\n",
    "cfg = manage_fine_tuning_config(\n",
    "            output_path, epoch_size, batch_size, freeze_layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Config:\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_experiment = False\n",
    "if \"discard_experiment\" in cfg:\n",
    "    discard_experiment = cfg[\"discard_experiment\"]\n",
    "cfg[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "the_metrics = []\n",
    "if cfg[\"metrics\"][\"use_metrics\"]:\n",
    "    get_metrics_fn_names = cfg[\"metrics\"][\"get_metrics_fn_names\"]\n",
    "    get_metrics_fn_parms = cfg[\"metrics\"][\"metrics_fn_parms\"]\n",
    "    assert len(get_metrics_fn_names) == len(get_metrics_fn_parms)\n",
    "    \n",
    "    \n",
    "    #---\n",
    "    # TODO: manually changing here for OHE experiment\n",
    "    # get_metrics_fn_names = [\"get_precision_fn\", \"get_iou_fn\"]\n",
    "    # get_metrics_fn_names = [\"get_sparse_categorical_accuracy_fn\", \"get_precision_fn\", \"get_iou_fn\", \"get_recall_fn\"]\n",
    "    # get_metrics_fn_parms = [{}, {}, {}, {}]\n",
    "    # get_metrics_fn_names = [\"get_precision_fn\", \"get_recall_fn\"]\n",
    "    get_metrics_fn_names = [ \"get_precision_fn\", \"get_iou_fn\", \"get_recall_fn\"]\n",
    "    get_metrics_fn_parms = [{}, {}, {}]\n",
    "    #---\n",
    "    for get_mf_name, mf_parms in zip(get_metrics_fn_names, get_metrics_fn_parms):\n",
    "        \n",
    "        get_metric_fn = getattr(metric_constructors, get_mf_name)\n",
    "        print(f\"Metric constructor function: {get_metric_fn.__name__}\")\n",
    "        metric_fn = get_metric_fn(mf_parms)\n",
    "        the_metrics.append(metric_fn)\n",
    "\n",
    "# specify a function that will construct the loss function\n",
    "get_loss_fn_name = cfg[\"loss\"][\"get_loss_fn_name\"]\n",
    "\n",
    "#---\n",
    "# TODO: manually changing here for OHE experiment\n",
    "# get_loss_fn_name = [\"get_sparse_categorical_crossentropy_fn\", \"get_categorical_crossentropy_fn\"]\n",
    "# get_loss_fn_parms = [{}, {}]\n",
    "get_loss_fn_name = [\"get_categorical_crossentropy_fn\"]\n",
    "get_loss_fn_parms = [{}]\n",
    "# ---\n",
    "\n",
    "# get_loss_fn = getattr(loss_constructors, get_loss_fn_name)\n",
    "# # Construct the loss function\n",
    "# loss_fn = get_loss_fn(cfg)\n",
    "\n",
    "# print(f\"Loss constructor function: {get_loss_fn.__name__}\")\n",
    "\n",
    "the_losses = []\n",
    "for get_lf_name, lf_parms in zip(get_loss_fn_name, get_loss_fn_parms):\n",
    "    \n",
    "    get_loss_fn = getattr(loss_constructors, get_lf_name)\n",
    "    print(f\"Loss constructor functions: {get_loss_fn.__name__}\")\n",
    "    loss_fn = get_loss_fn(lf_parms)\n",
    "    the_losses.append(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#### construct optimizer ####\n",
    "get_optimizer_fn_name = cfg[\"optimizer\"][\"get_optimizer_fn_name\"]\n",
    "get_optimizer_fn = getattr(optimizer_constructors, get_optimizer_fn_name)\n",
    "\n",
    "optimizer = get_optimizer_fn(cfg)\n",
    "\n",
    "the_model = None\n",
    "\n",
    "# SG: Using the saved model in this cell\n",
    "working_ramp_home = os.environ[\"RAMP_HOME\"]\n",
    "# load (construct) the model\n",
    "model_path = Path(working_ramp_home) / cfg[\"saved_model\"][\"saved_model_path\"]\n",
    "print(f\"Model: importing saved model {str(model_path)}\")\n",
    "the_model = tf.keras.models.load_model(model_path)\n",
    "assert (\n",
    "    the_model is not None\n",
    "), f\"the saved model was not constructed: {model_path}\"\n",
    "\n",
    "if cfg[\"freeze_layers\"]:\n",
    "    for layer in the_model.layers:\n",
    "        layer.trainable = False  # freeze previous layers only update new layers\n",
    "        # print(\"Setting previous model layers traininable : False\")\n",
    "\n",
    "\n",
    "print(\"-------\")\n",
    "print(f'-------{the_metrics}')\n",
    "print(\"-------\")\n",
    "\n",
    "# If you don't want to save the original state of training, recompile the model.\n",
    "the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[the_metrics])\n",
    "# the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[precision_class_0,precision_class_1])\n",
    "\n",
    "# the_model.compile(optimizer = optimizer,\n",
    "#    loss=loss_fn,\n",
    "#    metrics = [get_iou_coef_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of non-sparse loss fn with probs over labels and OHE labels (total 4 classes)\n",
    "y_true = tf.one_hot([0, 1], depth=4, axis=-1)\n",
    "y_pred = [[0.05, 0.95, 0, 0], [0.1, 0.8, 0.1, 0]]\n",
    "loss_fn(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss constructor function: {loss_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model.loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Model summary:\")\n",
    "# print(the_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# TODO: manually changing here for OHE experiment\n",
    "def ohe_batches(batches: tf.data.Dataset, depth=4) -> tf.data.Dataset:\n",
    "    \"\"\"For given batches and depth map sparse labels to OHE.\"\"\"\n",
    "    return batches.map(lambda x, y: (x, tf.one_hot(y[..., -1], depth, axis=-1)))\n",
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ramp.training.augmentation_constructors import get_augmentation_fn\n",
    "from ramp.utils.misc_ramp_utils import get_num_files\n",
    "from ramp.data_mgmt.data_generator import (\n",
    "    test_batches_from_gtiff_dirs,\n",
    "    training_batches_from_gtiff_dirs,\n",
    ")\n",
    "#### define data directories ####\n",
    "train_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_img_dir\"]\n",
    "train_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_mask_dir\"]\n",
    "val_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_img_dir\"]\n",
    "val_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_mask_dir\"]\n",
    "\n",
    "#### get the augmentation transform ####\n",
    "# aug = None\n",
    "if cfg[\"augmentation\"][\"use_aug\"]:\n",
    "    aug = get_augmentation_fn(cfg)\n",
    "\n",
    "## RUNTIME Parameters\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "input_img_shape = cfg[\"input_img_shape\"]\n",
    "output_img_shape = cfg[\"output_img_shape\"]\n",
    "\n",
    "n_training = get_num_files(train_img_dir, \"*.tif\")\n",
    "n_val = get_num_files(val_img_dir, \"*.tif\")\n",
    "steps_per_epoch = n_training // batch_size\n",
    "validation_steps = n_val // batch_size\n",
    "# Testing step , not recommended\n",
    "if validation_steps <= 0:\n",
    "    validation_steps = 1\n",
    "\n",
    "# add these back to the config\n",
    "# in case they are needed by callbacks\n",
    "cfg[\"runtime\"] = {}\n",
    "cfg[\"runtime\"][\"n_training\"] = n_training\n",
    "cfg[\"runtime\"][\"n_val\"] = n_val\n",
    "cfg[\"runtime\"][\"steps_per_epoch\"] = steps_per_epoch\n",
    "cfg[\"runtime\"][\"validation_steps\"] = validation_steps\n",
    "\n",
    "train_batches = None\n",
    "\n",
    "if aug is not None:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir,\n",
    "        train_mask_dir,\n",
    "        batch_size,\n",
    "        input_img_shape,\n",
    "        output_img_shape,\n",
    "        transforms=aug,\n",
    "    )\n",
    "else:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir, train_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    "    )\n",
    "assert train_batches is not None, \"training batches were not constructed\"\n",
    "print(f\"-------\\n* train img dir{train_img_dir}\\n* train mask dir{train_mask_dir}\")\n",
    "print(f\"* input img shape{input_img_shape}\\n* output img shape{output_img_shape}\")\n",
    "\n",
    "\n",
    "#---\n",
    "# TODO: manually changing here for OHE experiment\n",
    "train_batches = ohe_batches(train_batches)\n",
    "#---\n",
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches are a tf.data.Dataset type\n",
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a single batch as numpy to explore\n",
    "iter = tf.data.Dataset.as_numpy_iterator(train_batches)\n",
    "(X_batch, y_true_batch) = iter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elem in iter:\n",
    "#   print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true_batch.size # 524288\n",
    "# y_true_batch.itemsize # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.size # 393216\n",
    "# X_batch.itemsize # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_batch[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Print useful info\n",
    "print(\"Batch size is:\", X_batch.shape[0])\n",
    "# Appears to be floats between 0 and 1\n",
    "print(\"Example X entry:\", X_batch[0, 0, 0, :])\n",
    "print(\"Input data shape (X):\", X_batch.shape)\n",
    "print(\"Ground truth data shape (y_true):\", y_true_batch.shape)\n",
    "print(\"Example y_true: \", y_true_batch[1, 0, 0, :])\n",
    "print(f\"The unique labels in the y_true: {np.unique(y_true_batch[0, :, :, 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some predictions from the model for the batch\n",
    "y_pred_batch = the_model.predict(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of output predictions\n",
    "print(\"Predictions y_pred_batch are:\", y_pred_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example single pixel prediction\n",
    "single_prediction = y_pred_batch[0, 0, 0, :]\n",
    "print(single_prediction)\n",
    "\n",
    "# Single prediction sums to 1: probabilties over the four categories of the model\n",
    "single_prediction.sum().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model.loss(y_true_batch, y_pred_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation batches\n",
    "val_batches = test_batches_from_gtiff_dirs(\n",
    "    val_img_dir, val_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    ")\n",
    "\n",
    "#---\n",
    "# TODO: manually changing here for OHE experiment\n",
    "val_batches = ohe_batches(val_batches)\n",
    "#---\n",
    "\n",
    "assert val_batches is not None, \"validation batches were not constructed\"\n",
    "print(f\"-------\\n* val img dir{val_img_dir}\\n* val mask dir{val_mask_dir}\\n-------\")\n",
    "print(val_batches)\n",
    "print('*\\n*\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "# ---------------\n",
    "## Callbacks ##\n",
    "callbacks_list = []\n",
    "\n",
    "if not discard_experiment:\n",
    "    # get model checkpoint callback\n",
    "    if cfg[\"model_checkpts\"][\"use_model_checkpts\"]:\n",
    "        get_model_checkpt_callback_fn_name = cfg[\"model_checkpts\"][\n",
    "            \"get_model_checkpt_callback_fn_name\"\n",
    "        ]\n",
    "        get_model_checkpt_callback_fn = getattr(\n",
    "            callback_constructors, get_model_checkpt_callback_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_model_checkpt_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard callback\n",
    "    if cfg[\"tensorboard\"][\"use_tb\"]:\n",
    "        get_tb_callback_fn_name = cfg[\"tensorboard\"][\"get_tb_callback_fn_name\"]\n",
    "        get_tb_callback_fn = getattr(callback_constructors, get_tb_callback_fn_name)\n",
    "        callbacks_list.append(get_tb_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard model prediction logging callback\n",
    "    if cfg[\"prediction_logging\"][\"use_prediction_logging\"]:\n",
    "        assert cfg[\"tensorboard\"][\n",
    "            \"use_tb\"\n",
    "        ], \"Tensorboard logging must be turned on to enable prediction logging\"\n",
    "        get_prediction_logging_fn_name = cfg[\"prediction_logging\"][\n",
    "            \"get_prediction_logging_fn_name\"\n",
    "        ]\n",
    "        get_prediction_logging_fn = getattr(\n",
    "            callback_constructors, get_prediction_logging_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_prediction_logging_fn(the_model, cfg))\n",
    "\n",
    "# free up RAM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if cfg[\"early_stopping\"][\"use_early_stopping\"]:\n",
    "    callbacks_list.append(callback_constructors.get_early_stopping_callback_fn(cfg))\n",
    "\n",
    "    # get cyclic learning scheduler callback\n",
    "if cfg[\"cyclic_learning_scheduler\"][\"use_clr\"]:\n",
    "    assert not cfg[\"early_stopping\"][\n",
    "        \"use_early_stopping\"\n",
    "    ], \"cannot use early_stopping with cycling_learning_scheduler\"\n",
    "    get_clr_callback_fn_name = cfg[\"cyclic_learning_scheduler\"][\n",
    "        \"get_clr_callback_fn_name\"\n",
    "    ]\n",
    "    get_clr_callback_fn = getattr(callback_constructors, get_clr_callback_fn_name)\n",
    "    callbacks_list.append(get_clr_callback_fn(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call = callbacks_list[0]\n",
    "call.filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "## Main training block ##\n",
    "n_epochs = cfg[\"num_epochs\"]\n",
    "# SG: manually make this 10\n",
    "n_epochs = 5\n",
    "print(\n",
    "    f\"Starting Training with {n_epochs} epochs , {batch_size} batch size , {steps_per_epoch} steps per epoch , {validation_steps} validation steps......\"\n",
    ")\n",
    "if validation_steps <= 0:\n",
    "    raise RaiseError(\n",
    "        \"Not enough data for training, Increase image or Try reducing batchsize/epochs\"\n",
    "    )\n",
    "# FIXME : Make checkpoint\n",
    "start = perf_counter()\n",
    "history = the_model.fit(\n",
    "    train_batches,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_batches,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "end = perf_counter()\n",
    "print(f\"Training Finished , Time taken to train : {end-start} seconds\")\n",
    "print('\\n-----\\nHistory:')\n",
    "print(history.history.keys())\n",
    "print('\\n-----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n-----\\nHistory:')\n",
    "print(history.history.keys())\n",
    "\n",
    "print(f\"train iou {history.history['iou']}\")\n",
    "print(f\"valid iou {history.history['val_iou']}\")\n",
    "print(f'train recall {history.history[\"recall_1\"]}')\n",
    "print(f'valid recall {history.history[\"val_recall_1\"]}')\n",
    "print(f'train precision {history.history[\"precision_1\"]}')\n",
    "print(f'valid precision {history.history[\"val_precision_1\"]}')\n",
    "print('\\n-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f'{output_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m      2\u001b[0m history_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_output.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "history_df = pd.DataFrame.from_dict(history.history)\n",
    "history_df.to_csv(\"sample_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('my_history.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_saved=np.load('my_history.npy',allow_pickle='TRUE').item()\n",
    "history_saved.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_saved_df = pd.DataFrame.from_dict(history_saved)\n",
    "history_saved_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_last_df = pd.DataFrame.from_dict(history_saved)[-1:]\n",
    "history_last_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the training and validation accuracy and loss at each epoch\n",
    "print(\"Generating graphs ....\")\n",
    "if not os.path.exists(cfg[\"graph_location\"]):\n",
    "    os.mkdir(cfg[\"graph_location\"])\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# acc = history.history[\"sparse_categorical_accuracy\"]\n",
    "# val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "#---\n",
    "# TODO: updated for OHE\n",
    "acc = history.history[\"iou\"]\n",
    "val_acc = history.history[\"val_iou\"]\n",
    "acc1 = history.history[\"recall_1\"]\n",
    "val_acc1 = history.history[\"val_recall_1\"]\n",
    "acc2 = history.history[\"precision_1\"]\n",
    "val_acc2 = history.history[\"val_precision_1\"]\n",
    "#---\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(epochs, acc, \"y\", label=\"Training IoU\")\n",
    "plt.plot(epochs, val_acc, \"g\", label=\"Validation IoU\")\n",
    "plt.plot(epochs, acc1, \"r\", label=\"Training Recall\")\n",
    "plt.plot(epochs, val_acc1, \"m\", label=\"Validation Recall\")\n",
    "plt.plot(epochs, acc2, \"b\", label=\"Training Precision\")\n",
    "plt.plot(epochs, val_acc2, \"c\", label=\"Validation Precision\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "    f\"{cfg['graph_location']}/training_validation_sparse_categorical_accuracy.png\"\n",
    ")\n",
    "print(f\"Graph generated at : {cfg['graph_location']}\")\n",
    "# print(f\"train accuracy {acc}\")\n",
    "# print(f\"valid accuracy {val_acc}\")\n",
    "# print(f\"loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # APPENDIX: from copying across\n",
    "\n",
    "# #### construct optimizer ####\n",
    "# get_optimizer_fn_name = cfg[\"optimizer\"][\"get_optimizer_fn_name\"]\n",
    "# get_optimizer_fn = getattr(optimizer_constructors, get_optimizer_fn_name)\n",
    "\n",
    "# optimizer = get_optimizer_fn(cfg)\n",
    "\n",
    "# the_model = None\n",
    "\n",
    "# if cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "#     # load (construct) the model\n",
    "#     model_path = Path(working_ramp_home) / cfg[\"saved_model\"][\"saved_model_path\"]\n",
    "#     print(f\"Model: importing saved model {str(model_path)}\")\n",
    "#     the_model = tf.keras.models.load_model(model_path)\n",
    "#     assert (\n",
    "#         the_model is not None\n",
    "#     ), f\"the saved model was not constructed: {model_path}\"\n",
    "\n",
    "#     if cfg[\"freeze_layers\"]:\n",
    "#         for layer in the_model.layers:\n",
    "#             layer.trainable = False  # freeze previous layers only update new layers\n",
    "#             # print(\"Setting previous model layers traininable : False\")\n",
    "\n",
    "#     if not cfg[\"saved_model\"][\"save_optimizer_state\"]:\n",
    "#         print(\"-------\")\n",
    "#         print(f'-------{the_metrics}')\n",
    "#         print(\"-------\")\n",
    "        \n",
    "#         # For class 0\n",
    "#         precision_class_0 = Precision(class_id=0)\n",
    "#         # For class 1\n",
    "#         precision_class_1 = Precision(class_id=1)\n",
    "#         metrics=[precision_class_0,precision_class_1]\n",
    "#         print(f'-------{the_metrics}')\n",
    "#         print(\"-------\")\n",
    "        \n",
    "#         # If you don't want to save the original state of training, recompile the model.\n",
    "#         the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[the_metrics])\n",
    "#         # the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[precision_class_0,precision_class_1])\n",
    "        \n",
    "#         # the_model.compile(optimizer = optimizer,\n",
    "#         #    loss=loss_fn,\n",
    "#         #    metrics = [get_iou_coef_fn])\n",
    "\n",
    "# if not cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "#     get_model_fn_name = cfg[\"model\"][\"get_model_fn_name\"]\n",
    "#     get_model_fn = getattr(model_constructors, get_model_fn_name)\n",
    "#     the_model = get_model_fn(cfg)\n",
    "\n",
    "#     assert the_model is not None, f\"the model was not constructed: {model_path}\"\n",
    "#     the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=the_metrics)\n",
    "\n",
    "# print(the_model)\n",
    "# cfg[\"datasets\"]\n",
    "\n",
    "# #### define data directories ####\n",
    "# train_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_img_dir\"]\n",
    "# train_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_mask_dir\"]\n",
    "# val_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_img_dir\"]\n",
    "# val_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_mask_dir\"]\n",
    "\n",
    "# #### get the augmentation transform ####\n",
    "# # aug = None\n",
    "# if cfg[\"augmentation\"][\"use_aug\"]:\n",
    "#     aug = get_augmentation_fn(cfg)\n",
    "\n",
    "# ## RUNTIME Parameters\n",
    "# batch_size = cfg[\"batch_size\"]\n",
    "# input_img_shape = cfg[\"input_img_shape\"]\n",
    "# output_img_shape = cfg[\"output_img_shape\"]\n",
    "\n",
    "# n_training = get_num_files(train_img_dir, \"*.tif\")\n",
    "# n_val = get_num_files(val_img_dir, \"*.tif\")\n",
    "# steps_per_epoch = n_training // batch_size\n",
    "# validation_steps = n_val // batch_size\n",
    "# # Testing step , not recommended\n",
    "# if validation_steps <= 0:\n",
    "#     validation_steps = 1\n",
    "\n",
    "# # add these back to the config\n",
    "# # in case they are needed by callbacks\n",
    "# cfg[\"runtime\"] = {}\n",
    "# cfg[\"runtime\"][\"n_training\"] = n_training\n",
    "# cfg[\"runtime\"][\"n_val\"] = n_val\n",
    "# cfg[\"runtime\"][\"steps_per_epoch\"] = steps_per_epoch\n",
    "# cfg[\"runtime\"][\"validation_steps\"] = validation_steps\n",
    "\n",
    "# train_batches = None\n",
    "\n",
    "# if aug is not None:\n",
    "#     train_batches = training_batches_from_gtiff_dirs(\n",
    "#         train_img_dir,\n",
    "#         train_mask_dir,\n",
    "#         batch_size,\n",
    "#         input_img_shape,\n",
    "#         output_img_shape,\n",
    "#         transforms=aug,\n",
    "#     )\n",
    "# else:\n",
    "#     train_batches = training_batches_from_gtiff_dirs(\n",
    "#         train_img_dir, train_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    "#     )\n",
    "\n",
    "# assert train_batches is not None, \"training batches were not constructed\"\n",
    "# print(f\"-------\\n* train img dir{train_img_dir}\\n* train mask dir{train_mask_dir}\")\n",
    "# print(f\"* input img shape{input_img_shape}\\n* output img shape{output_img_shape}\")\n",
    "\n",
    "# print(train_batches)\n",
    "\n",
    "# val_batches = test_batches_from_gtiff_dirs(\n",
    "#     val_img_dir, val_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    "# )\n",
    "\n",
    "# assert val_batches is not None, \"validation batches were not constructed\"\n",
    "# print(f\"-------\\n* val img dir{val_img_dir}\\n* val mask dir{val_mask_dir}\\n-------\")\n",
    "# print(val_batches)\n",
    "# print('*\\n*\\n')\n",
    "\n",
    "# ## Callbacks ##\n",
    "# callbacks_list = []\n",
    "\n",
    "# if not discard_experiment:\n",
    "#     # get model checkpoint callback\n",
    "#     if cfg[\"model_checkpts\"][\"use_model_checkpts\"]:\n",
    "#         get_model_checkpt_callback_fn_name = cfg[\"model_checkpts\"][\n",
    "#             \"get_model_checkpt_callback_fn_name\"\n",
    "#         ]\n",
    "#         get_model_checkpt_callback_fn = getattr(\n",
    "#             callback_constructors, get_model_checkpt_callback_fn_name\n",
    "#         )\n",
    "#         callbacks_list.append(get_model_checkpt_callback_fn(cfg))\n",
    "\n",
    "#     # get tensorboard callback\n",
    "#     if cfg[\"tensorboard\"][\"use_tb\"]:\n",
    "#         get_tb_callback_fn_name = cfg[\"tensorboard\"][\"get_tb_callback_fn_name\"]\n",
    "#         get_tb_callback_fn = getattr(callback_constructors, get_tb_callback_fn_name)\n",
    "#         callbacks_list.append(get_tb_callback_fn(cfg))\n",
    "\n",
    "#     # get tensorboard model prediction logging callback\n",
    "#     if cfg[\"prediction_logging\"][\"use_prediction_logging\"]:\n",
    "#         assert cfg[\"tensorboard\"][\n",
    "#             \"use_tb\"\n",
    "#         ], \"Tensorboard logging must be turned on to enable prediction logging\"\n",
    "#         get_prediction_logging_fn_name = cfg[\"prediction_logging\"][\n",
    "#             \"get_prediction_logging_fn_name\"\n",
    "#         ]\n",
    "#         get_prediction_logging_fn = getattr(\n",
    "#             callback_constructors, get_prediction_logging_fn_name\n",
    "#         )\n",
    "#         callbacks_list.append(get_prediction_logging_fn(the_model, cfg))\n",
    "\n",
    "# # free up RAM\n",
    "# keras.backend.clear_session()\n",
    "\n",
    "# if cfg[\"early_stopping\"][\"use_early_stopping\"]:\n",
    "#     callbacks_list.append(callback_constructors.get_early_stopping_callback_fn(cfg))\n",
    "\n",
    "#     # get cyclic learning scheduler callback\n",
    "# if cfg[\"cyclic_learning_scheduler\"][\"use_clr\"]:\n",
    "#     assert not cfg[\"early_stopping\"][\n",
    "#         \"use_early_stopping\"\n",
    "#     ], \"cannot use early_stopping with cycling_learning_scheduler\"\n",
    "#     get_clr_callback_fn_name = cfg[\"cyclic_learning_scheduler\"][\n",
    "#         \"get_clr_callback_fn_name\"\n",
    "#     ]\n",
    "#     get_clr_callback_fn = getattr(callback_constructors, get_clr_callback_fn_name)\n",
    "#     callbacks_list.append(get_clr_callback_fn(cfg))\n",
    "\n",
    "# ## Main training block ##\n",
    "# n_epochs = cfg[\"num_epochs\"]\n",
    "# print(\n",
    "#     f\"Starting Training with {n_epochs} epochs , {batch_size} batch size , {steps_per_epoch} steps per epoch , {validation_steps} validation steps......\"\n",
    "# )\n",
    "# if validation_steps <= 0:\n",
    "#     raise RaiseError(\n",
    "#         \"Not enough data for training, Increase image or Try reducing batchsize/epochs\"\n",
    "#     )\n",
    "# # FIXME : Make checkpoint\n",
    "# start = perf_counter()\n",
    "# history = the_model.fit(\n",
    "#     train_batches,\n",
    "#     epochs=n_epochs,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     validation_data=val_batches,\n",
    "#     validation_steps=validation_steps,\n",
    "#     callbacks=callbacks_list,\n",
    "# )\n",
    "# end = perf_counter()\n",
    "# print(f\"Training Finished , Time taken to train : {end-start} seconds\")\n",
    "# print('\\n-----\\nHistory:')\n",
    "# print(history.history.keys())\n",
    "# print('\\n-----')\n",
    "\n",
    "# # plot the training and validation accuracy and loss at each epoch\n",
    "# print(\"Generating graphs ....\")\n",
    "# if not os.path.exists(cfg[\"graph_location\"]):\n",
    "#     os.mkdir(cfg[\"graph_location\"])\n",
    "\n",
    "# loss = history.history[\"loss\"]\n",
    "# # val_loss = history.history[\"val_loss\"]\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# acc = history.history[\"sparse_categorical_accuracy\"]\n",
    "# val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "# # Plot training and validation accuracy\n",
    "# plt.plot(epochs, acc, \"y\", label=\"Training Accuracy\")\n",
    "# plt.plot(epochs, val_acc, \"r\", label=\"Validation Accuracy\")\n",
    "\n",
    "# # Set labels and title\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.savefig(\n",
    "#     f\"{cfg['graph_location']}/training_validation_sparse_categorical_accuracy.png\"\n",
    "# )\n",
    "# print(f\"Graph generated at : {cfg['graph_location']}\")\n",
    "# print(f\"accuracy {acc}\")\n",
    "# print(f\"accuracy {val_acc}\")\n",
    "# print(f\"loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/fair_split_train.csv\")\n",
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hot_fair_utilities import train\n",
    "train_output = f\"{base_path}train\"\n",
    "final_accuracy, final_model_path = train(\n",
    "    input_path=preprocess_output,\n",
    "    output_path=train_output,\n",
    "    epoch_size=2,\n",
    "    batch_size=2,\n",
    "    model=\"ramp\",\n",
    "    model_home=os.environ[\"RAMP_HOME\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[here a tf file is created (weights + structure)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_accuracy,final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = f\"{os.getcwd()}/outputs/model51_td364/prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1PLe9S9BL8L",
    "outputId": "e4f3ce64-bbd6-4969-e49d-0f47dc9d6c0a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from hot_fair_utilities import predict\n",
    "\n",
    "\n",
    "print(f\"\\n**\\n** prediction output {prediction_output}\")\n",
    "print(f\"\\n**\\n** prediction input {base_path}prediction/input\")\n",
    "predict(\n",
    "    checkpoint_path=final_model_path,\n",
    "    input_path=f\"{base_path}prediction/input\",\n",
    "    prediction_path=prediction_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ho4zn_5UBgS3",
    "outputId": "afdec49e-4172-45a6-e7b6-cd34522ca7a0"
   },
   "outputs": [],
   "source": [
    "from hot_fair_utilities import polygonize\n",
    "geojson_output= f\"{prediction_output}/prediction.geojson\"\n",
    "polygonize(\n",
    "    input_path=prediction_output, \n",
    "    output_path=geojson_output,\n",
    "    remove_inputs = True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fairgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
