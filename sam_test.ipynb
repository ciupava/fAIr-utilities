{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/annazan/miniconda3/envs/fair38/lib/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getenv(\"LD_LIBRARY_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/home/annazan/miniconda3/envs/fair38/lib/\n",
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/home/annazan/miniconda3/envs/fair38/lib/\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH\n",
    "!source ~/.bashrc\n",
    "!echo $LD_LIBRARY_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WI8nslrSgROI",
    "outputId": "2cbad97c-765f-4d2b-87df-773447500332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/annazan/fAIr-utilities\n",
      "/home/annazan/fAIr-utilities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "os.environ.update(os.environ)\n",
    "        # Add a new environment variable to the operating system\n",
    "os.environ[\"RAMP_HOME\"] = os.getcwd()\n",
    "# Print the environment variables to verify that the new variable was added\n",
    "print(os.environ[\"RAMP_HOME\"])\n",
    "sys.path.append('../')\n",
    "sys.path.append('../ramp-code/')\n",
    "sys.path.append('ramp-code')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import cv2\n",
    "import ramp.utils\n",
    "import hot_fair_utilities\n",
    "\n",
    "# base_path = f\"{os.getcwd()}/ramp-data/sample_2\"\n",
    "# base_path = \"/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\"\n",
    "base_path = f\"{os.getcwd()}/ramp-data/test_data/1_Zanzibar\"\n",
    "model_input_image_path = f\"{base_path}/input\"\n",
    "preprocess_output=f\"{base_path}/preprocessed\"\n",
    "train_output = f\"{base_path}/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Georeferencing for input: 100%|██████████| 10/10 [00:00<00:00, 72.30it/s]\n",
      "Clipping labels for input: 100%|██████████| 10/10 [00:00<00:00, 19.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from hot_fair_utilities import preprocess\n",
    "\n",
    "preprocess(\n",
    "    input_path = model_input_image_path,\n",
    "    output_path = preprocess_output,\n",
    "    rasterize=True,\n",
    "    # rasterize_options=[\"binary\"],\n",
    "    rasterize_options=[\"binary\"],\n",
    "    georeference_images=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramp home is /home/annazan/fAIr-utilities\n",
      "python home is None\n",
      "variables are: src /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/preprocessed\n",
      " and dst:/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/fair_split_train.csv\n",
      "Writing /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/fair_split_val.csv\n"
     ]
    }
   ],
   "source": [
    "from hot_fair_utilities.training.prepare_data import split_training_2_validation\n",
    "x = split_training_2_validation(preprocess_output, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hot_fair_utilities.training.run_training import apply_feedback, manage_fine_tuning_config\n",
    "cfg = manage_fine_tuning_config(train_output, 2, 2, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from ramp.training import (\n",
    "    callback_constructors,\n",
    "    loss_constructors,\n",
    "    metric_constructors,\n",
    "    model_constructors,\n",
    "    optimizer_constructors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_experiment = False\n",
    "if \"discard_experiment\" in cfg:\n",
    "    discard_experiment = cfg[\"discard_experiment\"]\n",
    "cfg[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric constructor function: get_accuracy_fn\n"
     ]
    }
   ],
   "source": [
    "# specify a function that will construct the loss function\n",
    "get_loss_fn_name = cfg[\"loss\"][\"get_loss_fn_name\"]\n",
    "get_loss_fn = getattr(loss_constructors, get_loss_fn_name)\n",
    "# Construct the loss function\n",
    "loss_fn = get_loss_fn(cfg)\n",
    "\n",
    "the_metrics = []\n",
    "if cfg[\"metrics\"][\"use_metrics\"]:\n",
    "    get_metrics_fn_names = cfg[\"metrics\"][\"get_metrics_fn_names\"]\n",
    "    get_metrics_fn_parms = cfg[\"metrics\"][\"metrics_fn_parms\"]\n",
    "    assert len(get_metrics_fn_names) == len(get_metrics_fn_parms)\n",
    "    for get_mf_name, mf_parms in zip(get_metrics_fn_names, get_metrics_fn_parms):\n",
    "        get_metric_fn = getattr(metric_constructors, get_mf_name)\n",
    "        print(f\"Metric constructor function: {get_metric_fn.__name__}\")\n",
    "        metric_fn = get_metric_fn(mf_parms)\n",
    "        the_metrics.append(metric_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"saved_model\"][\"use_saved_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: importing saved model /home/annazan/fAIr-utilities/ramp-code/ramp/checkpoint.tf\n",
      "-------\n",
      "-------[<keras.metrics.metrics.Accuracy object at 0x7fc917b29d30>]\n",
      "-------\n",
      "-------[<keras.metrics.metrics.Accuracy object at 0x7fc917b29d30>]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import Precision\n",
    "from pathlib import Path\n",
    "\n",
    "#### construct optimizer ####\n",
    "get_optimizer_fn_name = cfg[\"optimizer\"][\"get_optimizer_fn_name\"]\n",
    "get_optimizer_fn = getattr(optimizer_constructors, get_optimizer_fn_name)\n",
    "\n",
    "optimizer = get_optimizer_fn(cfg)\n",
    "\n",
    "the_model = None\n",
    "\n",
    "# SG: Using the saved model in this cell\n",
    "working_ramp_home = os.environ[\"RAMP_HOME\"]\n",
    "# load (construct) the model\n",
    "model_path = Path(working_ramp_home) / cfg[\"saved_model\"][\"saved_model_path\"]\n",
    "print(f\"Model: importing saved model {str(model_path)}\")\n",
    "the_model = tf.keras.models.load_model(model_path)\n",
    "assert (\n",
    "    the_model is not None\n",
    "), f\"the saved model was not constructed: {model_path}\"\n",
    "\n",
    "if cfg[\"freeze_layers\"]:\n",
    "    for layer in the_model.layers:\n",
    "        layer.trainable = False  # freeze previous layers only update new layers\n",
    "        # print(\"Setting previous model layers traininable : False\")\n",
    "\n",
    "\n",
    "print(\"-------\")\n",
    "print(f'-------{the_metrics}')\n",
    "print(\"-------\")\n",
    "\n",
    "# For class 0\n",
    "precision_class_0 = Precision(class_id=0)\n",
    "# For class 1\n",
    "precision_class_1 = Precision(class_id=1)\n",
    "metrics=[precision_class_0,precision_class_1]\n",
    "print(f'-------{the_metrics}')\n",
    "print(\"-------\")\n",
    "\n",
    "# If you don't want to save the original state of training, recompile the model.\n",
    "the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[the_metrics])\n",
    "# the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[precision_class_0,precision_class_1])\n",
    "\n",
    "# the_model.compile(optimizer = optimizer,\n",
    "#    loss=loss_fn,\n",
    "#    metrics = [get_iou_coef_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg[\"freeze_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"saved_model\"][\"save_optimizer_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(the_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ramp.training.augmentation_constructors import get_augmentation_fn\n",
    "from ramp.utils.misc_ramp_utils import get_num_files\n",
    "from ramp.data_mgmt.data_generator import (\n",
    "    test_batches_from_gtiff_dirs,\n",
    "    training_batches_from_gtiff_dirs,\n",
    ")\n",
    "#### define data directories ####\n",
    "train_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_img_dir\"]\n",
    "train_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_mask_dir\"]\n",
    "val_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_img_dir\"]\n",
    "val_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_mask_dir\"]\n",
    "\n",
    "#### get the augmentation transform ####\n",
    "# aug = None\n",
    "if cfg[\"augmentation\"][\"use_aug\"]:\n",
    "    aug = get_augmentation_fn(cfg)\n",
    "\n",
    "## RUNTIME Parameters\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "input_img_shape = cfg[\"input_img_shape\"]\n",
    "output_img_shape = cfg[\"output_img_shape\"]\n",
    "\n",
    "n_training = get_num_files(train_img_dir, \"*.tif\")\n",
    "n_val = get_num_files(val_img_dir, \"*.tif\")\n",
    "steps_per_epoch = n_training // batch_size\n",
    "validation_steps = n_val // batch_size\n",
    "# Testing step , not recommended\n",
    "if validation_steps <= 0:\n",
    "    validation_steps = 1\n",
    "\n",
    "# add these back to the config\n",
    "# in case they are needed by callbacks\n",
    "cfg[\"runtime\"] = {}\n",
    "cfg[\"runtime\"][\"n_training\"] = n_training\n",
    "cfg[\"runtime\"][\"n_val\"] = n_val\n",
    "cfg[\"runtime\"][\"steps_per_epoch\"] = steps_per_epoch\n",
    "cfg[\"runtime\"][\"validation_steps\"] = validation_steps\n",
    "\n",
    "train_batches = None\n",
    "\n",
    "if aug is not None:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir,\n",
    "        train_mask_dir,\n",
    "        batch_size,\n",
    "        input_img_shape,\n",
    "        output_img_shape,\n",
    "        transforms=aug,\n",
    "    )\n",
    "else:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir, train_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    "    )\n",
    "assert train_batches is not None, \"training batches were not constructed\"\n",
    "print(f\"-------\\n* train img dir{train_img_dir}\\n* train mask dir{train_mask_dir}\")\n",
    "print(f\"* input img shape{input_img_shape}\\n* output img shape{output_img_shape}\")\n",
    "\n",
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.uint8, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Batches are a tf.data.Dataset type\n",
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch as numpy to explore\n",
    "(X_batch, y_true_batch) = tf.data.Dataset.as_numpy_iterator(train_batches).next()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size is: 2\n",
      "Example X entry: [0.35681677 0.34392428 0.27120593]\n",
      "Input data shape (X): (2, 256, 256, 3)\n",
      "Ground truth data shape (y_true): (2, 256, 256, 1)\n",
      "The unique labels in the y_true: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Print useful info\n",
    "print(\"Batch size is:\", X_batch.shape[0])\n",
    "# Appears to be floats between 0 and 1\n",
    "print(\"Example X entry:\", X_batch[0, 0, 0, :])\n",
    "print(\"Input data shape (X):\", X_batch.shape)\n",
    "print(\"Ground truth data shape (y_true):\", y_true_batch.shape)\n",
    "print(f\"The unique labels in the y_true: {np.unique(a_batch[1][0, :, :, 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get some predictions from the model for the batch\n",
    "y_pred_batch = the_model.predict(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions y_pred_batch are: (2, 256, 256, 4)\n"
     ]
    }
   ],
   "source": [
    "# Shape of output predictions\n",
    "print(\"Predictions y_pred_batch are:\", y_pred_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9449903e-01 2.8326476e-04 5.0833919e-03 1.3442575e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example single pixel prediction\n",
    "single_prediction = y_pred_batch[0, 0, 0, :]\n",
    "print(single_prediction)\n",
    "\n",
    "# Single prediction sums to 1: probabilties over the four categories of the model\n",
    "single_prediction.sum().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "* val img dir/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/val-chips\n",
      "* val mask dir/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/val-binarymasks\n",
      "-------\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.uint8, name=None))>\n",
      "*\n",
      "*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation batches\n",
    "val_batches = test_batches_from_gtiff_dirs(\n",
    "    val_img_dir, val_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    ")\n",
    "\n",
    "assert val_batches is not None, \"validation batches were not constructed\"\n",
    "print(f\"-------\\n* val img dir{val_img_dir}\\n* val mask dir{val_mask_dir}\\n-------\")\n",
    "print(val_batches)\n",
    "print('*\\n*\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "# ---------------\n",
    "## Callbacks ##\n",
    "callbacks_list = []\n",
    "\n",
    "if not discard_experiment:\n",
    "    # get model checkpoint callback\n",
    "    if cfg[\"model_checkpts\"][\"use_model_checkpts\"]:\n",
    "        get_model_checkpt_callback_fn_name = cfg[\"model_checkpts\"][\n",
    "            \"get_model_checkpt_callback_fn_name\"\n",
    "        ]\n",
    "        get_model_checkpt_callback_fn = getattr(\n",
    "            callback_constructors, get_model_checkpt_callback_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_model_checkpt_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard callback\n",
    "    if cfg[\"tensorboard\"][\"use_tb\"]:\n",
    "        get_tb_callback_fn_name = cfg[\"tensorboard\"][\"get_tb_callback_fn_name\"]\n",
    "        get_tb_callback_fn = getattr(callback_constructors, get_tb_callback_fn_name)\n",
    "        callbacks_list.append(get_tb_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard model prediction logging callback\n",
    "    if cfg[\"prediction_logging\"][\"use_prediction_logging\"]:\n",
    "        assert cfg[\"tensorboard\"][\n",
    "            \"use_tb\"\n",
    "        ], \"Tensorboard logging must be turned on to enable prediction logging\"\n",
    "        get_prediction_logging_fn_name = cfg[\"prediction_logging\"][\n",
    "            \"get_prediction_logging_fn_name\"\n",
    "        ]\n",
    "        get_prediction_logging_fn = getattr(\n",
    "            callback_constructors, get_prediction_logging_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_prediction_logging_fn(the_model, cfg))\n",
    "\n",
    "# free up RAM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if cfg[\"early_stopping\"][\"use_early_stopping\"]:\n",
    "    callbacks_list.append(callback_constructors.get_early_stopping_callback_fn(cfg))\n",
    "\n",
    "    # get cyclic learning scheduler callback\n",
    "if cfg[\"cyclic_learning_scheduler\"][\"use_clr\"]:\n",
    "    assert not cfg[\"early_stopping\"][\n",
    "        \"use_early_stopping\"\n",
    "    ], \"cannot use early_stopping with cycling_learning_scheduler\"\n",
    "    get_clr_callback_fn_name = cfg[\"cyclic_learning_scheduler\"][\n",
    "        \"get_clr_callback_fn_name\"\n",
    "    ]\n",
    "    get_clr_callback_fn = getattr(callback_constructors, get_clr_callback_fn_name)\n",
    "    callbacks_list.append(get_clr_callback_fn(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with 2 epochs , 2 batch size , 4 steps per epoch , 1 validation steps......\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 894, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 987, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 501, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/metrics/base_metric.py\", line 646, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/metrics/metrics.py\", line 3232, in accuracy  **\n        y_true.shape.assert_is_compatible_with(y_pred.shape)\n\n    ValueError: Shapes (None, 256, 256, 1) and (None, 256, 256, 4) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# FIXME : Make checkpoint\u001b[39;00m\n\u001b[1;32m     14\u001b[0m start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mthe_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m end \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Finished , Time taken to train : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filegtisgn_g.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 894, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 987, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 501, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/metrics/base_metric.py\", line 646, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/metrics/metrics.py\", line 3232, in accuracy  **\n        y_true.shape.assert_is_compatible_with(y_pred.shape)\n\n    ValueError: Shapes (None, 256, 256, 1) and (None, 256, 256, 4) are incompatible\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "## Main training block ##\n",
    "n_epochs = cfg[\"num_epochs\"]\n",
    "print(\n",
    "    f\"Starting Training with {n_epochs} epochs , {batch_size} batch size , {steps_per_epoch} steps per epoch , {validation_steps} validation steps......\"\n",
    ")\n",
    "if validation_steps <= 0:\n",
    "    raise RaiseError(\n",
    "        \"Not enough data for training, Increase image or Try reducing batchsize/epochs\"\n",
    "    )\n",
    "# FIXME : Make checkpoint\n",
    "start = perf_counter()\n",
    "history = the_model.fit(\n",
    "    train_batches,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_batches,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "end = perf_counter()\n",
    "print(f\"Training Finished , Time taken to train : {end-start} seconds\")\n",
    "print('\\n-----\\nHistory:')\n",
    "print(history.history.keys())\n",
    "print('\\n-----')\n",
    "\n",
    "# plot the training and validation accuracy and loss at each epoch\n",
    "print(\"Generating graphs ....\")\n",
    "if not os.path.exists(cfg[\"graph_location\"]):\n",
    "    os.mkdir(cfg[\"graph_location\"])\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "acc = history.history[\"sparse_categorical_accuracy\"]\n",
    "val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(epochs, acc, \"y\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, \"r\", label=\"Validation Accuracy\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "    f\"{cfg['graph_location']}/training_validation_sparse_categorical_accuracy.png\"\n",
    ")\n",
    "print(f\"Graph generated at : {cfg['graph_location']}\")\n",
    "print(f\"accuracy {acc}\")\n",
    "print(f\"accuracy {val_acc}\")\n",
    "print(f\"loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPENDIX: from copying across\n",
    "\n",
    "#### construct optimizer ####\n",
    "get_optimizer_fn_name = cfg[\"optimizer\"][\"get_optimizer_fn_name\"]\n",
    "get_optimizer_fn = getattr(optimizer_constructors, get_optimizer_fn_name)\n",
    "\n",
    "optimizer = get_optimizer_fn(cfg)\n",
    "\n",
    "the_model = None\n",
    "\n",
    "if cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "    # load (construct) the model\n",
    "    model_path = Path(working_ramp_home) / cfg[\"saved_model\"][\"saved_model_path\"]\n",
    "    print(f\"Model: importing saved model {str(model_path)}\")\n",
    "    the_model = tf.keras.models.load_model(model_path)\n",
    "    assert (\n",
    "        the_model is not None\n",
    "    ), f\"the saved model was not constructed: {model_path}\"\n",
    "\n",
    "    if cfg[\"freeze_layers\"]:\n",
    "        for layer in the_model.layers:\n",
    "            layer.trainable = False  # freeze previous layers only update new layers\n",
    "            # print(\"Setting previous model layers traininable : False\")\n",
    "\n",
    "    if not cfg[\"saved_model\"][\"save_optimizer_state\"]:\n",
    "        print(\"-------\")\n",
    "        print(f'-------{the_metrics}')\n",
    "        print(\"-------\")\n",
    "        \n",
    "        # For class 0\n",
    "        precision_class_0 = Precision(class_id=0)\n",
    "        # For class 1\n",
    "        precision_class_1 = Precision(class_id=1)\n",
    "        metrics=[precision_class_0,precision_class_1]\n",
    "        print(f'-------{the_metrics}')\n",
    "        print(\"-------\")\n",
    "        \n",
    "        # If you don't want to save the original state of training, recompile the model.\n",
    "        the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[the_metrics])\n",
    "        # the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[precision_class_0,precision_class_1])\n",
    "        \n",
    "        # the_model.compile(optimizer = optimizer,\n",
    "        #    loss=loss_fn,\n",
    "        #    metrics = [get_iou_coef_fn])\n",
    "\n",
    "if not cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "    get_model_fn_name = cfg[\"model\"][\"get_model_fn_name\"]\n",
    "    get_model_fn = getattr(model_constructors, get_model_fn_name)\n",
    "    the_model = get_model_fn(cfg)\n",
    "\n",
    "    assert the_model is not None, f\"the model was not constructed: {model_path}\"\n",
    "    the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=the_metrics)\n",
    "\n",
    "print(the_model)\n",
    "cfg[\"datasets\"]\n",
    "\n",
    "#### define data directories ####\n",
    "train_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_img_dir\"]\n",
    "train_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_mask_dir\"]\n",
    "val_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_img_dir\"]\n",
    "val_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_mask_dir\"]\n",
    "\n",
    "#### get the augmentation transform ####\n",
    "# aug = None\n",
    "if cfg[\"augmentation\"][\"use_aug\"]:\n",
    "    aug = get_augmentation_fn(cfg)\n",
    "\n",
    "## RUNTIME Parameters\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "input_img_shape = cfg[\"input_img_shape\"]\n",
    "output_img_shape = cfg[\"output_img_shape\"]\n",
    "\n",
    "n_training = get_num_files(train_img_dir, \"*.tif\")\n",
    "n_val = get_num_files(val_img_dir, \"*.tif\")\n",
    "steps_per_epoch = n_training // batch_size\n",
    "validation_steps = n_val // batch_size\n",
    "# Testing step , not recommended\n",
    "if validation_steps <= 0:\n",
    "    validation_steps = 1\n",
    "\n",
    "# add these back to the config\n",
    "# in case they are needed by callbacks\n",
    "cfg[\"runtime\"] = {}\n",
    "cfg[\"runtime\"][\"n_training\"] = n_training\n",
    "cfg[\"runtime\"][\"n_val\"] = n_val\n",
    "cfg[\"runtime\"][\"steps_per_epoch\"] = steps_per_epoch\n",
    "cfg[\"runtime\"][\"validation_steps\"] = validation_steps\n",
    "\n",
    "train_batches = None\n",
    "\n",
    "if aug is not None:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir,\n",
    "        train_mask_dir,\n",
    "        batch_size,\n",
    "        input_img_shape,\n",
    "        output_img_shape,\n",
    "        transforms=aug,\n",
    "    )\n",
    "else:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir, train_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    "    )\n",
    "\n",
    "assert train_batches is not None, \"training batches were not constructed\"\n",
    "print(f\"-------\\n* train img dir{train_img_dir}\\n* train mask dir{train_mask_dir}\")\n",
    "print(f\"* input img shape{input_img_shape}\\n* output img shape{output_img_shape}\")\n",
    "\n",
    "print(train_batches)\n",
    "\n",
    "val_batches = test_batches_from_gtiff_dirs(\n",
    "    val_img_dir, val_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    ")\n",
    "\n",
    "assert val_batches is not None, \"validation batches were not constructed\"\n",
    "print(f\"-------\\n* val img dir{val_img_dir}\\n* val mask dir{val_mask_dir}\\n-------\")\n",
    "print(val_batches)\n",
    "print('*\\n*\\n')\n",
    "\n",
    "## Callbacks ##\n",
    "callbacks_list = []\n",
    "\n",
    "if not discard_experiment:\n",
    "    # get model checkpoint callback\n",
    "    if cfg[\"model_checkpts\"][\"use_model_checkpts\"]:\n",
    "        get_model_checkpt_callback_fn_name = cfg[\"model_checkpts\"][\n",
    "            \"get_model_checkpt_callback_fn_name\"\n",
    "        ]\n",
    "        get_model_checkpt_callback_fn = getattr(\n",
    "            callback_constructors, get_model_checkpt_callback_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_model_checkpt_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard callback\n",
    "    if cfg[\"tensorboard\"][\"use_tb\"]:\n",
    "        get_tb_callback_fn_name = cfg[\"tensorboard\"][\"get_tb_callback_fn_name\"]\n",
    "        get_tb_callback_fn = getattr(callback_constructors, get_tb_callback_fn_name)\n",
    "        callbacks_list.append(get_tb_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard model prediction logging callback\n",
    "    if cfg[\"prediction_logging\"][\"use_prediction_logging\"]:\n",
    "        assert cfg[\"tensorboard\"][\n",
    "            \"use_tb\"\n",
    "        ], \"Tensorboard logging must be turned on to enable prediction logging\"\n",
    "        get_prediction_logging_fn_name = cfg[\"prediction_logging\"][\n",
    "            \"get_prediction_logging_fn_name\"\n",
    "        ]\n",
    "        get_prediction_logging_fn = getattr(\n",
    "            callback_constructors, get_prediction_logging_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_prediction_logging_fn(the_model, cfg))\n",
    "\n",
    "# free up RAM\n",
    "keras.backend.clear_session()\n",
    "\n",
    "if cfg[\"early_stopping\"][\"use_early_stopping\"]:\n",
    "    callbacks_list.append(callback_constructors.get_early_stopping_callback_fn(cfg))\n",
    "\n",
    "    # get cyclic learning scheduler callback\n",
    "if cfg[\"cyclic_learning_scheduler\"][\"use_clr\"]:\n",
    "    assert not cfg[\"early_stopping\"][\n",
    "        \"use_early_stopping\"\n",
    "    ], \"cannot use early_stopping with cycling_learning_scheduler\"\n",
    "    get_clr_callback_fn_name = cfg[\"cyclic_learning_scheduler\"][\n",
    "        \"get_clr_callback_fn_name\"\n",
    "    ]\n",
    "    get_clr_callback_fn = getattr(callback_constructors, get_clr_callback_fn_name)\n",
    "    callbacks_list.append(get_clr_callback_fn(cfg))\n",
    "\n",
    "## Main training block ##\n",
    "n_epochs = cfg[\"num_epochs\"]\n",
    "print(\n",
    "    f\"Starting Training with {n_epochs} epochs , {batch_size} batch size , {steps_per_epoch} steps per epoch , {validation_steps} validation steps......\"\n",
    ")\n",
    "if validation_steps <= 0:\n",
    "    raise RaiseError(\n",
    "        \"Not enough data for training, Increase image or Try reducing batchsize/epochs\"\n",
    "    )\n",
    "# FIXME : Make checkpoint\n",
    "start = perf_counter()\n",
    "history = the_model.fit(\n",
    "    train_batches,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_batches,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "end = perf_counter()\n",
    "print(f\"Training Finished , Time taken to train : {end-start} seconds\")\n",
    "print('\\n-----\\nHistory:')\n",
    "print(history.history.keys())\n",
    "print('\\n-----')\n",
    "\n",
    "# plot the training and validation accuracy and loss at each epoch\n",
    "print(\"Generating graphs ....\")\n",
    "if not os.path.exists(cfg[\"graph_location\"]):\n",
    "    os.mkdir(cfg[\"graph_location\"])\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "acc = history.history[\"sparse_categorical_accuracy\"]\n",
    "val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(epochs, acc, \"y\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, \"r\", label=\"Validation Accuracy\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "    f\"{cfg['graph_location']}/training_validation_sparse_categorical_accuracy.png\"\n",
    ")\n",
    "print(f\"Graph generated at : {cfg['graph_location']}\")\n",
    "print(f\"accuracy {acc}\")\n",
    "print(f\"accuracy {val_acc}\")\n",
    "print(f\"loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/chips/OAM-319293-270962-19.tif'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/fair_split_train.csv\")\n",
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables are /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\n",
      " and /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain\n",
      "Starting to prepare data for training\n",
      "ramp home is /home/annazan/fAIr-utilities\n",
      "python home is None\n",
      "variables are: src /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\n",
      " and dst:/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py\", line 128, in <module>\n",
      "    main()\n",
      "  File \"/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py\", line 91, in main\n",
      "    raise ValueError(f\"source directory {src_dir} is not readable\")\n",
      "ValueError: source directory /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/chips is not readable\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', '/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py', '-src', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/chips', '-pfx', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/fair_split', '-trn', '0.85', '-val', '0.15']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m final_accuracy, final_model_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mramp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_home\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRAMP_HOME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fAIr-utilities/hot_fair_utilities/training/train.py:54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_path, output_path, epoch_size, batch_size, model, model_home, freeze_layers)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables are \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m input_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m output_path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to prepare data for training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43msplit_training_2_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone split\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m cfg \u001b[38;5;241m=\u001b[39m manage_fine_tuning_config(\n\u001b[1;32m     57\u001b[0m     output_path, epoch_size, batch_size, freeze_layers\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/fAIr-utilities/hot_fair_utilities/training/prepare_data.py:66\u001b[0m, in \u001b[0;36msplit_training_2_validation\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mcheck_output(\n\u001b[1;32m     51\u001b[0m         [\n\u001b[1;32m     52\u001b[0m             python_exec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m         env\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron,\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# move all the VALIDATION chips, labels and masks to their new locations\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/fAIr-utilities/hot_fair_utilities/training/prepare_data.py:50\u001b[0m, in \u001b[0;36msplit_training_2_validation\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Define the script as a string\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# SPLIT INTO TRAINING AND VALIDATION\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# script = f\"\"\"%run ramp-code/scripts/make_train_val_split_lists.py -src {dst_path}/chips -pfx {uid}_fair_split -trn 0.85 -val 0.15\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpython_exec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mRAMP_HOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ramp-code/scripts/make_train_val_split_lists.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-src\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdst_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/chips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-pfx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdst_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/fair_split\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-trn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.85\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.15\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/subprocess.py:415\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m         empty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    413\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/subprocess.py:516\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    517\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['python', '/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py', '-src', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/chips', '-pfx', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/fair_split', '-trn', '0.85', '-val', '0.15']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "from hot_fair_utilities import train\n",
    "train_output = f\"{base_path}train\"\n",
    "final_accuracy, final_model_path = train(\n",
    "    input_path=preprocess_output,\n",
    "    output_path=train_output,\n",
    "    epoch_size=2,\n",
    "    batch_size=2,\n",
    "    model=\"ramp\",\n",
    "    model_home=os.environ[\"RAMP_HOME\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[here a tf file is created (weights + structure)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_accuracy,final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = f\"{os.getcwd()}/outputs/model51_td364/prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1PLe9S9BL8L",
    "outputId": "e4f3ce64-bbd6-4969-e49d-0f47dc9d6c0a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from hot_fair_utilities import predict\n",
    "\n",
    "\n",
    "print(f\"\\n**\\n** prediction output {prediction_output}\")\n",
    "print(f\"\\n**\\n** prediction input {base_path}prediction/input\")\n",
    "predict(\n",
    "    checkpoint_path=final_model_path,\n",
    "    input_path=f\"{base_path}prediction/input\",\n",
    "    prediction_path=prediction_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ho4zn_5UBgS3",
    "outputId": "afdec49e-4172-45a6-e7b6-cd34522ca7a0"
   },
   "outputs": [],
   "source": [
    "from hot_fair_utilities import polygonize\n",
    "geojson_output= f\"{prediction_output}/prediction.geojson\"\n",
    "polygonize(\n",
    "    input_path=prediction_output, \n",
    "    output_path=geojson_output,\n",
    "    remove_inputs = True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fairgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
