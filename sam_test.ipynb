{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':/home/annazan/miniconda3/envs/fair38/lib/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getenv(\"LD_LIBRARY_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ":/home/annazan/miniconda3/envs/fair38/lib/\n",
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      ":/home/annazan/miniconda3/envs/fair38/lib/\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH\n",
    "!source ~/.bashrc\n",
    "!echo $LD_LIBRARY_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WI8nslrSgROI",
    "outputId": "2cbad97c-765f-4d2b-87df-773447500332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/annazan/fAIr-utilities\n",
      "/home/annazan/fAIr-utilities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "os.environ.update(os.environ)\n",
    "        # Add a new environment variable to the operating system\n",
    "os.environ[\"RAMP_HOME\"] = os.getcwd()\n",
    "# Print the environment variables to verify that the new variable was added\n",
    "print(os.environ[\"RAMP_HOME\"])\n",
    "sys.path.append('../')\n",
    "sys.path.append('../ramp-code/')\n",
    "sys.path.append('ramp-code')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import cv2\n",
    "import ramp.utils\n",
    "import hot_fair_utilities\n",
    "\n",
    "# base_path = f\"{os.getcwd()}/ramp-data/sample_2\"\n",
    "# base_path = \"/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\"\n",
    "# base_path = f\"{os.getcwd()}/ramp-data/test_data/1_Zanzibar\"\n",
    "base_path = f'{os.getcwd()}/ramp-data/test_data/model95_td370/'\n",
    "model_input_image_path = f\"{base_path}/input\"\n",
    "preprocess_output=f\"{base_path}/preprocessed\"\n",
    "train_output = f\"{base_path}/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hot_fair_utilities import preprocess\n",
    "\n",
    "# preprocess(\n",
    "#     input_path = model_input_image_path,\n",
    "#     output_path = preprocess_output,\n",
    "#     rasterize=True,\n",
    "#     # rasterize_options=[\"binary\"],\n",
    "#     rasterize_options=[\"binary\"],\n",
    "#     georeference_images=True,\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "mkdir: cannot create directory ‘ramp-data/test_data/1_Zanzibar/preprocessed/multimasks/’: File exists\n",
      "/bin/bash: /home/annazan/miniconda3/envs/fair38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Input polygons directory: ramp-data/test_data/1_Zanzibar/preprocessed/labels/\n",
      "Input matching image chips directory: ramp-data/test_data/1_Zanzibar/preprocessed/chips/\n",
      "Output multichannel masks directory: ramp-data/test_data/1_Zanzibar/preprocessed/multimasks/\n",
      "Boundary mask boundary width: 2\n",
      "Contact mask contact spacing: 4\n",
      "10it [00:00, 73071.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Make multimask path\n",
    "# !mkdir ramp-data/test_data/1_Zanzibar/preprocessed/multimasks/\n",
    "# # Run script for multi-mask: https://github.com/kshitijrajsharma/ramp-code-fAIr/blob/ae33b11364f0a61f278ce9ff93446586704ea275/scripts/multi_masks_from_polygons.py\n",
    "# !python ramp-code/scripts/multi_masks_from_polygons.py -in_vecs ramp-data/test_data/1_Zanzibar/preprocessed/labels/ -in_chips ramp-data/test_data/1_Zanzibar/preprocessed/chips/ -out ramp-data/test_data/1_Zanzibar/preprocessed/multimasks/ -bwidth 2 -csp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramp home is /home/annazan/fAIr-utilities\n",
      "python home is None\n",
      "variables are: src /home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/\n",
      " and dst:/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370//train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing /home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370//train/fair_split_train.csv\n",
      "Writing /home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370//train/fair_split_val.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess_output=f\"{base_path}\"\n",
    "\n",
    "from hot_fair_utilities.training.prepare_data import split_training_2_validation\n",
    "x = split_training_2_validation(preprocess_output, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from ramp.training import (\n",
    "    callback_constructors,\n",
    "    loss_constructors,\n",
    "    metric_constructors,\n",
    "    model_constructors,\n",
    "    optimizer_constructors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hot_fair_utilities.training.run_training import manage_fine_tuning_config\n",
    "\n",
    "output_path=train_output\n",
    "epoch_size=2\n",
    "batch_size=2\n",
    "freeze_layers=False\n",
    "cfg = manage_fine_tuning_config(\n",
    "            output_path, epoch_size, batch_size, freeze_layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_experiment = False\n",
    "if \"discard_experiment\" in cfg:\n",
    "    discard_experiment = cfg[\"discard_experiment\"]\n",
    "cfg[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric constructor function: get_precision_fn\n"
     ]
    }
   ],
   "source": [
    "# specify a function that will construct the loss function\n",
    "get_loss_fn_name = cfg[\"loss\"][\"get_loss_fn_name\"]\n",
    "get_loss_fn = getattr(loss_constructors, get_loss_fn_name)\n",
    "# Construct the loss function\n",
    "loss_fn = get_loss_fn(cfg)\n",
    "\n",
    "the_metrics = []\n",
    "if cfg[\"metrics\"][\"use_metrics\"]:\n",
    "    get_metrics_fn_names = cfg[\"metrics\"][\"get_metrics_fn_names\"]\n",
    "    get_metrics_fn_parms = cfg[\"metrics\"][\"metrics_fn_parms\"]\n",
    "    assert len(get_metrics_fn_names) == len(get_metrics_fn_parms)\n",
    "    for get_mf_name, mf_parms in zip(get_metrics_fn_names, get_metrics_fn_parms):\n",
    "        get_metric_fn = getattr(metric_constructors, get_mf_name)\n",
    "        print(f\"Metric constructor function: {get_metric_fn.__name__}\")\n",
    "        metric_fn = get_metric_fn(mf_parms)\n",
    "        the_metrics.append(metric_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"saved_model\"][\"use_saved_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: importing saved model /home/annazan/fAIr-utilities/ramp-code/ramp/checkpoint.tf\n",
      "-------\n",
      "-------[<keras.metrics.metrics.Precision object at 0x7fd38a52b4c0>]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#### construct optimizer ####\n",
    "get_optimizer_fn_name = cfg[\"optimizer\"][\"get_optimizer_fn_name\"]\n",
    "get_optimizer_fn = getattr(optimizer_constructors, get_optimizer_fn_name)\n",
    "\n",
    "optimizer = get_optimizer_fn(cfg)\n",
    "\n",
    "the_model = None\n",
    "\n",
    "# SG: Using the saved model in this cell\n",
    "working_ramp_home = os.environ[\"RAMP_HOME\"]\n",
    "# load (construct) the model\n",
    "model_path = Path(working_ramp_home) / cfg[\"saved_model\"][\"saved_model_path\"]\n",
    "print(f\"Model: importing saved model {str(model_path)}\")\n",
    "the_model = tf.keras.models.load_model(model_path)\n",
    "assert (\n",
    "    the_model is not None\n",
    "), f\"the saved model was not constructed: {model_path}\"\n",
    "\n",
    "if cfg[\"freeze_layers\"]:\n",
    "    for layer in the_model.layers:\n",
    "        layer.trainable = False  # freeze previous layers only update new layers\n",
    "        # print(\"Setting previous model layers traininable : False\")\n",
    "\n",
    "\n",
    "print(\"-------\")\n",
    "print(f'-------{the_metrics}')\n",
    "print(\"-------\")\n",
    "\n",
    "# If you don't want to save the original state of training, recompile the model.\n",
    "the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[the_metrics])\n",
    "# the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[precision_class_0,precision_class_1])\n",
    "\n",
    "# the_model.compile(optimizer = optimizer,\n",
    "#    loss=loss_fn,\n",
    "#    metrics = [get_iou_coef_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=float32, numpy=\n",
       "array([[[2.9957325 ]],\n",
       "\n",
       "       [[0.22314365]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of loss fn with multiple prediction labels but only binary true labels\n",
    "y_true = [[[[0, 1]]]]\n",
    "y_pred = [[[[0.05, 0.95, 0, 0]]], [[[0.1, 0.8, 0.1, 0]]]]\n",
    "loss_fn(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1), dtype=float32, numpy=\n",
       "array([[[2.9957325 ]],\n",
       "\n",
       "       [[0.22314365]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cfg[\"freeze_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"saved_model\"][\"save_optimizer_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(the_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "* train img dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/chips\n",
      "* train mask dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/binarymasks\n",
      "* input img shape[256, 256]\n",
      "* output img shape[256, 256]\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 4), dtype=tf.uint8, name=None))>\n"
     ]
    }
   ],
   "source": [
    "from ramp.training.augmentation_constructors import get_augmentation_fn\n",
    "from ramp.utils.misc_ramp_utils import get_num_files\n",
    "from ramp.data_mgmt.data_generator import (\n",
    "    test_batches_from_gtiff_dirs,\n",
    "    training_batches_from_gtiff_dirs,\n",
    ")\n",
    "#### define data directories ####\n",
    "train_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_img_dir\"]\n",
    "train_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_mask_dir\"]\n",
    "val_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_img_dir\"]\n",
    "val_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_mask_dir\"]\n",
    "\n",
    "#### get the augmentation transform ####\n",
    "# aug = None\n",
    "if cfg[\"augmentation\"][\"use_aug\"]:\n",
    "    aug = get_augmentation_fn(cfg)\n",
    "\n",
    "## RUNTIME Parameters\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "input_img_shape = cfg[\"input_img_shape\"]\n",
    "output_img_shape = cfg[\"output_img_shape\"]\n",
    "\n",
    "n_training = get_num_files(train_img_dir, \"*.tif\")\n",
    "n_val = get_num_files(val_img_dir, \"*.tif\")\n",
    "steps_per_epoch = n_training // batch_size\n",
    "validation_steps = n_val // batch_size\n",
    "# Testing step , not recommended\n",
    "if validation_steps <= 0:\n",
    "    validation_steps = 1\n",
    "\n",
    "# add these back to the config\n",
    "# in case they are needed by callbacks\n",
    "cfg[\"runtime\"] = {}\n",
    "cfg[\"runtime\"][\"n_training\"] = n_training\n",
    "cfg[\"runtime\"][\"n_val\"] = n_val\n",
    "cfg[\"runtime\"][\"steps_per_epoch\"] = steps_per_epoch\n",
    "cfg[\"runtime\"][\"validation_steps\"] = validation_steps\n",
    "\n",
    "train_batches = None\n",
    "\n",
    "if aug is not None:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir,\n",
    "        train_mask_dir,\n",
    "        batch_size,\n",
    "        input_img_shape,\n",
    "        output_img_shape,\n",
    "        transforms=aug,\n",
    "    )\n",
    "else:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir, train_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    "    )\n",
    "assert train_batches is not None, \"training batches were not constructed\"\n",
    "print(f\"-------\\n* train img dir{train_img_dir}\\n* train mask dir{train_mask_dir}\")\n",
    "print(f\"* input img shape{input_img_shape}\\n* output img shape{output_img_shape}\")\n",
    "\n",
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 4), dtype=tf.uint8, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Batches are a tf.data.Dataset type\n",
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "error: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n\nTraceback (most recent call last):\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/__autograph_generated_filefke4o_n2.py\", line 40, in apply_augmentation\n    ag__.if_stmt((ag__.ld(transforms) is not None), if_body, else_body, get_state, set_state, ('aug_img', 'aug_mask'), 2)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1341, in if_stmt\n    _py_if_stmt(cond, body, orelse)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1394, in _py_if_stmt\n    return body() if cond else orelse()\n\n  File \"/tmp/__autograph_generated_filefke4o_n2.py\", line 29, in if_body\n    aug_data = ag__.converted_call(ag__.ld(transforms), (), dict(**ag__.ld(data)), fscope_1)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 458, in _call_unconverted\n    return f(*args, **kwargs)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/core/composition.py\", line 191, in __call__\n    data = t(force_apply=force_apply, **data)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 90, in __call__\n    return self.apply_with_params(params, **kwargs)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 103, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/geometric/rotate.py\", line 91, in apply_to_mask\n    return F.rotate(img, angle, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/functional.py\", line 70, in wrapped_function\n    result = func(img, *args, **kwargs)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/geometric/functional.py\", line 79, in rotate\n    return warp_fn(img)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/functional.py\", line 189, in __process_fn\n    img = process_fn(img, **kwargs)\n\ncv2.error: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get a single batch as numpy to explore\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mas_numpy_iterator(train_batches)\n\u001b[0;32m----> 3\u001b[0m (X_batch, y_true_batch) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4481\u001b[0m, in \u001b[0;36m_NumpyIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 4481\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4478\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4475\u001b[0m     numpy\u001b[38;5;241m.\u001b[39msetflags(write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   4476\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m numpy\n\u001b[0;32m-> 4478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(to_numpy, \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    765\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3017\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3016\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3017\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3019\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mUnknownError\u001b[0m: error: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n\nTraceback (most recent call last):\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/__autograph_generated_filefke4o_n2.py\", line 40, in apply_augmentation\n    ag__.if_stmt((ag__.ld(transforms) is not None), if_body, else_body, get_state, set_state, ('aug_img', 'aug_mask'), 2)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1341, in if_stmt\n    _py_if_stmt(cond, body, orelse)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1394, in _py_if_stmt\n    return body() if cond else orelse()\n\n  File \"/tmp/__autograph_generated_filefke4o_n2.py\", line 29, in if_body\n    aug_data = ag__.converted_call(ag__.ld(transforms), (), dict(**ag__.ld(data)), fscope_1)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 458, in _call_unconverted\n    return f(*args, **kwargs)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/core/composition.py\", line 191, in __call__\n    data = t(force_apply=force_apply, **data)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 90, in __call__\n    return self.apply_with_params(params, **kwargs)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 103, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/geometric/rotate.py\", line 91, in apply_to_mask\n    return F.rotate(img, angle, cv2.INTER_NEAREST, self.border_mode, self.mask_value)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/functional.py\", line 70, in wrapped_function\n    result = func(img, *args, **kwargs)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/geometric/functional.py\", line 79, in rotate\n    return warp_fn(img)\n\n  File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/albumentations/augmentations/functional.py\", line 189, in __process_fn\n    img = process_fn(img, **kwargs)\n\ncv2.error: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'\n\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# Get a single batch as numpy to explore\n",
    "iter = tf.data.Dataset.as_numpy_iterator(train_batches)\n",
    "(X_batch, y_true_batch) = iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Print useful info\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size is:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_batch\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Appears to be floats between 0 and 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample X entry:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_batch[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_batch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Print useful info\n",
    "print(\"Batch size is:\", X_batch.shape[0])\n",
    "# Appears to be floats between 0 and 1\n",
    "print(\"Example X entry:\", X_batch[0, 0, 0, :])\n",
    "print(\"Input data shape (X):\", X_batch.shape)\n",
    "print(\"Ground truth data shape (y_true):\", y_true_batch.shape)\n",
    "print(\"Example y_true: \", y_true_batch[1, 0, 0, :])\n",
    "print(f\"The unique labels in the y_true: {np.unique(y_true_batch[0, :, :, 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get some predictions from the model for the batch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred_batch \u001b[38;5;241m=\u001b[39m the_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_batch\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Get some predictions from the model for the batch\n",
    "y_pred_batch = the_model.predict(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Shape of output predictions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions y_pred_batch are:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43my_pred_batch\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Shape of output predictions\n",
    "print(\"Predictions y_pred_batch are:\", y_pred_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example single pixel prediction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m single_prediction \u001b[38;5;241m=\u001b[39m \u001b[43my_pred_batch\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(single_prediction)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Single prediction sums to 1: probabilties over the four categories of the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Example single pixel prediction\n",
    "single_prediction = y_pred_batch[0, 0, 0, :]\n",
    "print(single_prediction)\n",
    "\n",
    "# Single prediction sums to 1: probabilties over the four categories of the model\n",
    "single_prediction.sum().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m the_model\u001b[38;5;241m.\u001b[39mloss(\u001b[43my_true_batch\u001b[49m, y_pred_batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true_batch' is not defined"
     ]
    }
   ],
   "source": [
    "the_model.loss(y_true_batch, y_pred_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "* val img dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/val-chips\n",
      "* val mask dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/val-binarymasks\n",
      "-------\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>\n",
      "*\n",
      "*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation batches\n",
    "val_batches = test_batches_from_gtiff_dirs(\n",
    "    val_img_dir, val_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    ")\n",
    "\n",
    "assert val_batches is not None, \"validation batches were not constructed\"\n",
    "print(f\"-------\\n* val img dir{val_img_dir}\\n* val mask dir{val_mask_dir}\\n-------\")\n",
    "print(val_batches)\n",
    "print('*\\n*\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "# ---------------\n",
    "## Callbacks ##\n",
    "callbacks_list = []\n",
    "\n",
    "if not discard_experiment:\n",
    "    # get model checkpoint callback\n",
    "    if cfg[\"model_checkpts\"][\"use_model_checkpts\"]:\n",
    "        get_model_checkpt_callback_fn_name = cfg[\"model_checkpts\"][\n",
    "            \"get_model_checkpt_callback_fn_name\"\n",
    "        ]\n",
    "        get_model_checkpt_callback_fn = getattr(\n",
    "            callback_constructors, get_model_checkpt_callback_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_model_checkpt_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard callback\n",
    "    if cfg[\"tensorboard\"][\"use_tb\"]:\n",
    "        get_tb_callback_fn_name = cfg[\"tensorboard\"][\"get_tb_callback_fn_name\"]\n",
    "        get_tb_callback_fn = getattr(callback_constructors, get_tb_callback_fn_name)\n",
    "        callbacks_list.append(get_tb_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard model prediction logging callback\n",
    "    if cfg[\"prediction_logging\"][\"use_prediction_logging\"]:\n",
    "        assert cfg[\"tensorboard\"][\n",
    "            \"use_tb\"\n",
    "        ], \"Tensorboard logging must be turned on to enable prediction logging\"\n",
    "        get_prediction_logging_fn_name = cfg[\"prediction_logging\"][\n",
    "            \"get_prediction_logging_fn_name\"\n",
    "        ]\n",
    "        get_prediction_logging_fn = getattr(\n",
    "            callback_constructors, get_prediction_logging_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_prediction_logging_fn(the_model, cfg))\n",
    "\n",
    "# free up RAM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if cfg[\"early_stopping\"][\"use_early_stopping\"]:\n",
    "    callbacks_list.append(callback_constructors.get_early_stopping_callback_fn(cfg))\n",
    "\n",
    "    # get cyclic learning scheduler callback\n",
    "if cfg[\"cyclic_learning_scheduler\"][\"use_clr\"]:\n",
    "    assert not cfg[\"early_stopping\"][\n",
    "        \"use_early_stopping\"\n",
    "    ], \"cannot use early_stopping with cycling_learning_scheduler\"\n",
    "    get_clr_callback_fn_name = cfg[\"cyclic_learning_scheduler\"][\n",
    "        \"get_clr_callback_fn_name\"\n",
    "    ]\n",
    "    get_clr_callback_fn = getattr(callback_constructors, get_clr_callback_fn_name)\n",
    "    callbacks_list.append(get_clr_callback_fn(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with 10 epochs , 2 batch size , 71 steps per epoch , 12 validation steps......\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n      app.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5903/1721190905.py\", line 17, in <module>\n      history = the_model.fit(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1039, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n      app.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5903/1721190905.py\", line 17, in <module>\n      history = the_model.fit(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1039, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Incompatible shapes at component 1: expected [?,256,256,4] but got [2,256,256,256,4].\n\t [[{{node IteratorGetNext}}]]\n\t [[assert_less_equal/Assert/AssertGuard/else/_11/assert_less_equal/Assert/AssertGuard/Assert/data_0/_144]]\n  (1) INVALID_ARGUMENT:  Incompatible shapes at component 1: expected [?,256,256,4] but got [2,256,256,256,4].\n\t [[{{node IteratorGetNext}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_52014]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# FIXME : Make checkpoint\u001b[39;00m\n\u001b[1;32m     16\u001b[0m start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mthe_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m end \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Finished , Time taken to train : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n      app.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5903/1721190905.py\", line 17, in <module>\n      history = the_model.fit(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1039, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/traitlets/config/application.py\", line 1077, in launch_instance\n      app.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5903/1721190905.py\", line 17, in <module>\n      history = the_model.fit(\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/home/annazan/miniconda3/envs/fair38/lib/python3.8/site-packages/keras/engine/training.py\", line 1039, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Incompatible shapes at component 1: expected [?,256,256,4] but got [2,256,256,256,4].\n\t [[{{node IteratorGetNext}}]]\n\t [[assert_less_equal/Assert/AssertGuard/else/_11/assert_less_equal/Assert/AssertGuard/Assert/data_0/_144]]\n  (1) INVALID_ARGUMENT:  Incompatible shapes at component 1: expected [?,256,256,4] but got [2,256,256,256,4].\n\t [[{{node IteratorGetNext}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_52014]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "## Main training block ##\n",
    "n_epochs = cfg[\"num_epochs\"]\n",
    "# SG: manually make this 10\n",
    "n_epochs = 10\n",
    "print(\n",
    "    f\"Starting Training with {n_epochs} epochs , {batch_size} batch size , {steps_per_epoch} steps per epoch , {validation_steps} validation steps......\"\n",
    ")\n",
    "if validation_steps <= 0:\n",
    "    raise RaiseError(\n",
    "        \"Not enough data for training, Increase image or Try reducing batchsize/epochs\"\n",
    "    )\n",
    "# FIXME : Make checkpoint\n",
    "start = perf_counter()\n",
    "history = the_model.fit(\n",
    "    train_batches,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_batches,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "end = perf_counter()\n",
    "print(f\"Training Finished , Time taken to train : {end-start} seconds\")\n",
    "print('\\n-----\\nHistory:')\n",
    "print(history.history.keys())\n",
    "print('\\n-----')\n",
    "\n",
    "# plot the training and validation accuracy and loss at each epoch\n",
    "print(\"Generating graphs ....\")\n",
    "if not os.path.exists(cfg[\"graph_location\"]):\n",
    "    os.mkdir(cfg[\"graph_location\"])\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "acc = history.history[\"sparse_categorical_accuracy\"]\n",
    "val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(epochs, acc, \"y\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, \"r\", label=\"Validation Accuracy\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "    f\"{cfg['graph_location']}/training_validation_sparse_categorical_accuracy.png\"\n",
    ")\n",
    "print(f\"Graph generated at : {cfg['graph_location']}\")\n",
    "print(f\"accuracy {acc}\")\n",
    "print(f\"accuracy {val_acc}\")\n",
    "print(f\"loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: importing saved model /home/annazan/fAIr-utilities/ramp-code/ramp/checkpoint.tf\n",
      "-------\n",
      "-------[<keras.metrics.metrics.MeanIoU object at 0x7f2082ef02e0>]\n",
      "-------\n",
      "-------[<keras.metrics.metrics.MeanIoU object at 0x7f2082ef02e0>]\n",
      "-------\n",
      "<keras.engine.functional.Functional object at 0x7f1c7873e0d0>\n",
      "-------\n",
      "* train img dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/chips\n",
      "* train mask dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/binarymasks\n",
      "* input img shape[256, 256]\n",
      "* output img shape[256, 256]\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.uint8, name=None))>\n",
      "-------\n",
      "* val img dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/val-chips\n",
      "* val mask dir/home/annazan/fAIr-utilities/ramp-data/test_data/model95_td370/train/val-binarymasks\n",
      "-------\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.uint8, name=None))>\n",
      "*\n",
      "*\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m         callbacks_list\u001b[38;5;241m.\u001b[39mappend(get_prediction_logging_fn(the_model, cfg))\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# free up RAM\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearly_stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_early_stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    157\u001b[0m     callbacks_list\u001b[38;5;241m.\u001b[39mappend(callback_constructors\u001b[38;5;241m.\u001b[39mget_early_stopping_callback_fn(cfg))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# APPENDIX: from copying across\n",
    "\n",
    "#### construct optimizer ####\n",
    "get_optimizer_fn_name = cfg[\"optimizer\"][\"get_optimizer_fn_name\"]\n",
    "get_optimizer_fn = getattr(optimizer_constructors, get_optimizer_fn_name)\n",
    "\n",
    "optimizer = get_optimizer_fn(cfg)\n",
    "\n",
    "the_model = None\n",
    "\n",
    "if cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "    # load (construct) the model\n",
    "    model_path = Path(working_ramp_home) / cfg[\"saved_model\"][\"saved_model_path\"]\n",
    "    print(f\"Model: importing saved model {str(model_path)}\")\n",
    "    the_model = tf.keras.models.load_model(model_path)\n",
    "    assert (\n",
    "        the_model is not None\n",
    "    ), f\"the saved model was not constructed: {model_path}\"\n",
    "\n",
    "    if cfg[\"freeze_layers\"]:\n",
    "        for layer in the_model.layers:\n",
    "            layer.trainable = False  # freeze previous layers only update new layers\n",
    "            # print(\"Setting previous model layers traininable : False\")\n",
    "\n",
    "    if not cfg[\"saved_model\"][\"save_optimizer_state\"]:\n",
    "        print(\"-------\")\n",
    "        print(f'-------{the_metrics}')\n",
    "        print(\"-------\")\n",
    "        \n",
    "        # For class 0\n",
    "        precision_class_0 = Precision(class_id=0)\n",
    "        # For class 1\n",
    "        precision_class_1 = Precision(class_id=1)\n",
    "        metrics=[precision_class_0,precision_class_1]\n",
    "        print(f'-------{the_metrics}')\n",
    "        print(\"-------\")\n",
    "        \n",
    "        # If you don't want to save the original state of training, recompile the model.\n",
    "        the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[the_metrics])\n",
    "        # the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[precision_class_0,precision_class_1])\n",
    "        \n",
    "        # the_model.compile(optimizer = optimizer,\n",
    "        #    loss=loss_fn,\n",
    "        #    metrics = [get_iou_coef_fn])\n",
    "\n",
    "if not cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "    get_model_fn_name = cfg[\"model\"][\"get_model_fn_name\"]\n",
    "    get_model_fn = getattr(model_constructors, get_model_fn_name)\n",
    "    the_model = get_model_fn(cfg)\n",
    "\n",
    "    assert the_model is not None, f\"the model was not constructed: {model_path}\"\n",
    "    the_model.compile(optimizer=optimizer, loss=loss_fn, metrics=the_metrics)\n",
    "\n",
    "print(the_model)\n",
    "cfg[\"datasets\"]\n",
    "\n",
    "#### define data directories ####\n",
    "train_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_img_dir\"]\n",
    "train_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"train_mask_dir\"]\n",
    "val_img_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_img_dir\"]\n",
    "val_mask_dir = Path(working_ramp_home) / cfg[\"datasets\"][\"val_mask_dir\"]\n",
    "\n",
    "#### get the augmentation transform ####\n",
    "# aug = None\n",
    "if cfg[\"augmentation\"][\"use_aug\"]:\n",
    "    aug = get_augmentation_fn(cfg)\n",
    "\n",
    "## RUNTIME Parameters\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "input_img_shape = cfg[\"input_img_shape\"]\n",
    "output_img_shape = cfg[\"output_img_shape\"]\n",
    "\n",
    "n_training = get_num_files(train_img_dir, \"*.tif\")\n",
    "n_val = get_num_files(val_img_dir, \"*.tif\")\n",
    "steps_per_epoch = n_training // batch_size\n",
    "validation_steps = n_val // batch_size\n",
    "# Testing step , not recommended\n",
    "if validation_steps <= 0:\n",
    "    validation_steps = 1\n",
    "\n",
    "# add these back to the config\n",
    "# in case they are needed by callbacks\n",
    "cfg[\"runtime\"] = {}\n",
    "cfg[\"runtime\"][\"n_training\"] = n_training\n",
    "cfg[\"runtime\"][\"n_val\"] = n_val\n",
    "cfg[\"runtime\"][\"steps_per_epoch\"] = steps_per_epoch\n",
    "cfg[\"runtime\"][\"validation_steps\"] = validation_steps\n",
    "\n",
    "train_batches = None\n",
    "\n",
    "if aug is not None:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir,\n",
    "        train_mask_dir,\n",
    "        batch_size,\n",
    "        input_img_shape,\n",
    "        output_img_shape,\n",
    "        transforms=aug,\n",
    "    )\n",
    "else:\n",
    "    train_batches = training_batches_from_gtiff_dirs(\n",
    "        train_img_dir, train_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    "    )\n",
    "\n",
    "assert train_batches is not None, \"training batches were not constructed\"\n",
    "print(f\"-------\\n* train img dir{train_img_dir}\\n* train mask dir{train_mask_dir}\")\n",
    "print(f\"* input img shape{input_img_shape}\\n* output img shape{output_img_shape}\")\n",
    "\n",
    "print(train_batches)\n",
    "\n",
    "val_batches = test_batches_from_gtiff_dirs(\n",
    "    val_img_dir, val_mask_dir, batch_size, input_img_shape, output_img_shape\n",
    ")\n",
    "\n",
    "assert val_batches is not None, \"validation batches were not constructed\"\n",
    "print(f\"-------\\n* val img dir{val_img_dir}\\n* val mask dir{val_mask_dir}\\n-------\")\n",
    "print(val_batches)\n",
    "print('*\\n*\\n')\n",
    "\n",
    "## Callbacks ##\n",
    "callbacks_list = []\n",
    "\n",
    "if not discard_experiment:\n",
    "    # get model checkpoint callback\n",
    "    if cfg[\"model_checkpts\"][\"use_model_checkpts\"]:\n",
    "        get_model_checkpt_callback_fn_name = cfg[\"model_checkpts\"][\n",
    "            \"get_model_checkpt_callback_fn_name\"\n",
    "        ]\n",
    "        get_model_checkpt_callback_fn = getattr(\n",
    "            callback_constructors, get_model_checkpt_callback_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_model_checkpt_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard callback\n",
    "    if cfg[\"tensorboard\"][\"use_tb\"]:\n",
    "        get_tb_callback_fn_name = cfg[\"tensorboard\"][\"get_tb_callback_fn_name\"]\n",
    "        get_tb_callback_fn = getattr(callback_constructors, get_tb_callback_fn_name)\n",
    "        callbacks_list.append(get_tb_callback_fn(cfg))\n",
    "\n",
    "    # get tensorboard model prediction logging callback\n",
    "    if cfg[\"prediction_logging\"][\"use_prediction_logging\"]:\n",
    "        assert cfg[\"tensorboard\"][\n",
    "            \"use_tb\"\n",
    "        ], \"Tensorboard logging must be turned on to enable prediction logging\"\n",
    "        get_prediction_logging_fn_name = cfg[\"prediction_logging\"][\n",
    "            \"get_prediction_logging_fn_name\"\n",
    "        ]\n",
    "        get_prediction_logging_fn = getattr(\n",
    "            callback_constructors, get_prediction_logging_fn_name\n",
    "        )\n",
    "        callbacks_list.append(get_prediction_logging_fn(the_model, cfg))\n",
    "\n",
    "# free up RAM\n",
    "keras.backend.clear_session()\n",
    "\n",
    "if cfg[\"early_stopping\"][\"use_early_stopping\"]:\n",
    "    callbacks_list.append(callback_constructors.get_early_stopping_callback_fn(cfg))\n",
    "\n",
    "    # get cyclic learning scheduler callback\n",
    "if cfg[\"cyclic_learning_scheduler\"][\"use_clr\"]:\n",
    "    assert not cfg[\"early_stopping\"][\n",
    "        \"use_early_stopping\"\n",
    "    ], \"cannot use early_stopping with cycling_learning_scheduler\"\n",
    "    get_clr_callback_fn_name = cfg[\"cyclic_learning_scheduler\"][\n",
    "        \"get_clr_callback_fn_name\"\n",
    "    ]\n",
    "    get_clr_callback_fn = getattr(callback_constructors, get_clr_callback_fn_name)\n",
    "    callbacks_list.append(get_clr_callback_fn(cfg))\n",
    "\n",
    "## Main training block ##\n",
    "n_epochs = cfg[\"num_epochs\"]\n",
    "print(\n",
    "    f\"Starting Training with {n_epochs} epochs , {batch_size} batch size , {steps_per_epoch} steps per epoch , {validation_steps} validation steps......\"\n",
    ")\n",
    "if validation_steps <= 0:\n",
    "    raise RaiseError(\n",
    "        \"Not enough data for training, Increase image or Try reducing batchsize/epochs\"\n",
    "    )\n",
    "# FIXME : Make checkpoint\n",
    "start = perf_counter()\n",
    "history = the_model.fit(\n",
    "    train_batches,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_batches,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "end = perf_counter()\n",
    "print(f\"Training Finished , Time taken to train : {end-start} seconds\")\n",
    "print('\\n-----\\nHistory:')\n",
    "print(history.history.keys())\n",
    "print('\\n-----')\n",
    "\n",
    "# plot the training and validation accuracy and loss at each epoch\n",
    "print(\"Generating graphs ....\")\n",
    "if not os.path.exists(cfg[\"graph_location\"]):\n",
    "    os.mkdir(cfg[\"graph_location\"])\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "acc = history.history[\"sparse_categorical_accuracy\"]\n",
    "val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(epochs, acc, \"y\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, \"r\", label=\"Validation Accuracy\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "    f\"{cfg['graph_location']}/training_validation_sparse_categorical_accuracy.png\"\n",
    ")\n",
    "print(f\"Graph generated at : {cfg['graph_location']}\")\n",
    "print(f\"accuracy {acc}\")\n",
    "print(f\"accuracy {val_acc}\")\n",
    "print(f\"loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/chips/OAM-319293-270962-19.tif'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar/train/fair_split_train.csv\")\n",
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables are /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\n",
      " and /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain\n",
      "Starting to prepare data for training\n",
      "ramp home is /home/annazan/fAIr-utilities\n",
      "python home is None\n",
      "variables are: src /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibar\n",
      " and dst:/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py\", line 128, in <module>\n",
      "    main()\n",
      "  File \"/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py\", line 91, in main\n",
      "    raise ValueError(f\"source directory {src_dir} is not readable\")\n",
      "ValueError: source directory /home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/chips is not readable\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', '/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py', '-src', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/chips', '-pfx', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/fair_split', '-trn', '0.85', '-val', '0.15']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m final_accuracy, final_model_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mramp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_home\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRAMP_HOME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fAIr-utilities/hot_fair_utilities/training/train.py:54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_path, output_path, epoch_size, batch_size, model, model_home, freeze_layers)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables are \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m input_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m output_path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to prepare data for training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43msplit_training_2_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone split\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m cfg \u001b[38;5;241m=\u001b[39m manage_fine_tuning_config(\n\u001b[1;32m     57\u001b[0m     output_path, epoch_size, batch_size, freeze_layers\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/fAIr-utilities/hot_fair_utilities/training/prepare_data.py:66\u001b[0m, in \u001b[0;36msplit_training_2_validation\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mcheck_output(\n\u001b[1;32m     51\u001b[0m         [\n\u001b[1;32m     52\u001b[0m             python_exec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m         env\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron,\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# move all the VALIDATION chips, labels and masks to their new locations\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/fAIr-utilities/hot_fair_utilities/training/prepare_data.py:50\u001b[0m, in \u001b[0;36msplit_training_2_validation\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Define the script as a string\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# SPLIT INTO TRAINING AND VALIDATION\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# script = f\"\"\"%run ramp-code/scripts/make_train_val_split_lists.py -src {dst_path}/chips -pfx {uid}_fair_split -trn 0.85 -val 0.15\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpython_exec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mRAMP_HOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ramp-code/scripts/make_train_val_split_lists.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-src\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdst_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/chips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-pfx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdst_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/fair_split\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-trn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.85\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.15\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/subprocess.py:415\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m         empty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    413\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[0;32m~/miniconda3/envs/fair38/lib/python3.8/subprocess.py:516\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    517\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['python', '/home/annazan/fAIr-utilities/ramp-code/scripts/make_train_val_split_lists.py', '-src', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/chips', '-pfx', '/home/annazan/fAIr-utilities/ramp-data/test_data/1_Zanzibartrain/fair_split', '-trn', '0.85', '-val', '0.15']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "from hot_fair_utilities import train\n",
    "train_output = f\"{base_path}train\"\n",
    "final_accuracy, final_model_path = train(\n",
    "    input_path=preprocess_output,\n",
    "    output_path=train_output,\n",
    "    epoch_size=2,\n",
    "    batch_size=2,\n",
    "    model=\"ramp\",\n",
    "    model_home=os.environ[\"RAMP_HOME\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[here a tf file is created (weights + structure)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_accuracy,final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = f\"{os.getcwd()}/outputs/model51_td364/prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1PLe9S9BL8L",
    "outputId": "e4f3ce64-bbd6-4969-e49d-0f47dc9d6c0a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from hot_fair_utilities import predict\n",
    "\n",
    "\n",
    "print(f\"\\n**\\n** prediction output {prediction_output}\")\n",
    "print(f\"\\n**\\n** prediction input {base_path}prediction/input\")\n",
    "predict(\n",
    "    checkpoint_path=final_model_path,\n",
    "    input_path=f\"{base_path}prediction/input\",\n",
    "    prediction_path=prediction_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ho4zn_5UBgS3",
    "outputId": "afdec49e-4172-45a6-e7b6-cd34522ca7a0"
   },
   "outputs": [],
   "source": [
    "from hot_fair_utilities import polygonize\n",
    "geojson_output= f\"{prediction_output}/prediction.geojson\"\n",
    "polygonize(\n",
    "    input_path=prediction_output, \n",
    "    output_path=geojson_output,\n",
    "    remove_inputs = True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fairgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
